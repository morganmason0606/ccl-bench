Running on: nid003860
GPUs allocated: 0,1,2,3
=== Running experiment: llama3_4gpu_tp_2_dp2 ===
Model=meta-llama/Llama-3.1-8B DP=2 TP=2 PP=1 GPUs=0,1,2,3
Trace dir: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2
Server PID: 1772573
INFO 12-19 17:37:53 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:37:53 [__init__.py:241] Automatically detected platform cuda.
WARNING 12-19 17:38:06 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:38:06 [__init__.py:1750] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:06 [api_server.py:1875] vLLM API server version 0.10.1rc2.dev263+g2f13319f4
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:06 [utils.py:326] non-default args: {'model': 'meta-llama/Llama-3.1-8B', 'trust_remote_code': True, 'enforce_eager': True, 'disable_sliding_window': True, 'tensor_parallel_size': 2, 'data_parallel_size': 2, 'swap_space': 16.0, 'max_num_batched_tokens': 512, 'max_num_seqs': 512, 'enable_chunked_prefill': True}
[1;36m(APIServer pid=1772573)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
WARNING 12-19 17:38:08 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0d03e70b80>, seed=0, num_prompts=50, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=2048, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, endpoint_type='openai', label=None, backend='vllm', base_url='http://127.0.0.1:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=10.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=True, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600)
INFO 12-19 17:38:10 [datasets.py:509] Sampling input_len from [2047, 2047] and output_len from [512, 512]
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
 |          | 00:00 elapsed, ? remaining |          | 00:00 elapsed, 00:00 remaining |          | 00:05 elapsed, 09:54 remaining |          | 00:05 elapsed, 09:54 remaining |‚ñè         | 00:10 elapsed, 09:49 remaining |‚ñè         | 00:10 elapsed, 09:49 remaining |‚ñé         | 00:15 elapsed, 09:44 remaining |‚ñé         | 00:15 elapsed, 09:44 remaining |‚ñé         | 00:20 elapsed, 09:39 remaining |‚ñé         | 00:20 elapsed, 09:39 remaining |‚ñç         | 00:25 elapsed, 09:34 remaining |‚ñç         | 00:25 elapsed, 09:34 remaining |‚ñå         | 00:30 elapsed, 09:29 remaining |‚ñå         | 00:30 elapsed, 09:29 remaining |‚ñå         | 00:35 elapsed, 09:24 remaining |‚ñå         | 00:35 elapsed, 09:24 remaining[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:48 [__init__.py:742] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1772573)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:48 [__init__.py:1786] Using max model len 131072
 |‚ñã         | 00:40 elapsed, 09:19 remaining |‚ñã         | 00:40 elapsed, 09:19 remaining[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:51 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=512.
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:51 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:38:52 [utils.py:625] Started DP Coordinator process (PID: 1773295)
 |‚ñä         | 00:45 elapsed, 09:14 remaining |‚ñä         | 00:45 elapsed, 09:14 remaining |‚ñä         | 00:50 elapsed, 09:09 remaining |‚ñä         | 00:50 elapsed, 09:09 remaining |‚ñâ         | 00:55 elapsed, 09:04 remaining |‚ñâ         | 00:55 elapsed, 09:04 remaining |‚ñà         | 01:00 elapsed, 08:59 remaining |‚ñà         | 01:00 elapsed, 08:59 remaining |‚ñà         | 01:05 elapsed, 08:54 remaining |‚ñà         | 01:05 elapsed, 08:54 remaining |‚ñà‚ñè        | 01:10 elapsed, 08:49 remaining |‚ñà‚ñè        | 01:10 elapsed, 08:49 remainingINFO 12-19 17:39:25 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:39:25 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:39:25 [__init__.py:241] Automatically detected platform cuda.
 |‚ñà‚ñé        | 01:15 elapsed, 08:44 remaining |‚ñà‚ñé        | 01:15 elapsed, 08:44 remaining |‚ñà‚ñé        | 01:20 elapsed, 08:39 remaining |‚ñà‚ñé        | 01:20 elapsed, 08:39 remainingWARNING 12-19 17:39:34 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:39:34 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:39:34 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:39:34 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:39:34 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:39:35 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev263+g2f13319f4) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:39:35 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev263+g2f13319f4) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=1773298)[0;0m WARNING 12-19 17:39:35 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_1 pid=1773299)[0;0m WARNING 12-19 17:39:35 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:39:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_d8cf89dd'), local_subscribe_addr='ipc:///tmp/f9d384d0-8fc8-4f49-ba9a-5ab522146024', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:39:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_c133144e'), local_subscribe_addr='ipc:///tmp/3db3cf60-c2e6-43e3-8727-90c1ae6095b9', remote_subscribe_addr=None, remote_addr_ipv6=False)
 |‚ñà‚ñç        | 01:25 elapsed, 08:34 remaining |‚ñà‚ñç        | 01:25 elapsed, 08:34 remaining |‚ñà‚ñå        | 01:30 elapsed, 08:29 remaining |‚ñà‚ñå        | 01:30 elapsed, 08:29 remaining |‚ñà‚ñå        | 01:35 elapsed, 08:24 remaining |‚ñà‚ñå        | 01:35 elapsed, 08:24 remaining |‚ñà‚ñã        | 01:40 elapsed, 08:19 remaining |‚ñà‚ñã        | 01:40 elapsed, 08:19 remaining |‚ñà‚ñä        | 01:45 elapsed, 08:14 remaining |‚ñà‚ñä        | 01:45 elapsed, 08:14 remaining |‚ñà‚ñä        | 01:50 elapsed, 08:09 remaining |‚ñà‚ñä        | 01:50 elapsed, 08:09 remaining |‚ñà‚ñâ        | 01:55 elapsed, 08:04 remaining |‚ñà‚ñâ        | 01:55 elapsed, 08:04 remainingINFO 12-19 17:40:09 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:40:09 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:40:09 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:40:09 [__init__.py:241] Automatically detected platform cuda.
 |‚ñà‚ñà        | 02:00 elapsed, 07:59 remaining |‚ñà‚ñà        | 02:00 elapsed, 07:59 remaining |‚ñà‚ñà        | 02:05 elapsed, 07:54 remaining |‚ñà‚ñà        | 02:05 elapsed, 07:54 remainingWARNING 12-19 17:40:19 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:40:19 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:40:19 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:40:19 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
 |‚ñà‚ñà‚ñè       | 02:10 elapsed, 07:49 remaining |‚ñà‚ñà‚ñè       | 02:10 elapsed, 07:49 remaining |‚ñà‚ñà‚ñé       | 02:15 elapsed, 07:44 remaining |‚ñà‚ñà‚ñé       | 02:15 elapsed, 07:44 remainingINFO 12-19 17:40:26 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2
INFO 12-19 17:40:26 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2
INFO 12-19 17:40:26 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_68870e11'), local_subscribe_addr='ipc:///tmp/f0bd4941-82ba-4b57-8db7-9b78355cdc58', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6f3feac0'), local_subscribe_addr='ipc:///tmp/dad3b0fe-6c2f-486b-ae63-3bbe02bdea82', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_94f5abee'), local_subscribe_addr='ipc:///tmp/2a1d4708-6777-4048-89fa-106468f3ef2c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-19 17:40:26 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e9eaf069'), local_subscribe_addr='ipc:///tmp/962f3f4a-75f6-4e06-bc50-2454e784eeb3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:27 [parallel_state.py:992] Adjusting world_size=4 rank=1 distributed_init_method=tcp://127.0.0.1:34125 for DP
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:27 [parallel_state.py:992] Adjusting world_size=4 rank=0 distributed_init_method=tcp://127.0.0.1:34125 for DP
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:27 [parallel_state.py:992] Adjusting world_size=4 rank=2 distributed_init_method=tcp://127.0.0.1:34125 for DP
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:27 [parallel_state.py:992] Adjusting world_size=4 rank=3 distributed_init_method=tcp://127.0.0.1:34125 for DP
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:28 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:28 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:28 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:28 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:28 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:28 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:28 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:28 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:28 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:28 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_d37178e5'), local_subscribe_addr='ipc:///tmp/8c629607-dde1-4d04-9209-bc52439de1dd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:28 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:28 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_1cc16052'), local_subscribe_addr='ipc:///tmp/8127dfa0-4222-4f58-ba44-3227937e2810', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:29 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:29 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:29 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:29 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:29 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:29 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:29 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:29 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:29 [parallel_state.py:1134] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:29 [parallel_state.py:1134] rank 3 in world size 4 is assigned as DP rank 1, PP rank 0, TP rank 1, EP rank 3
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:29 [parallel_state.py:1134] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:29 [parallel_state.py:1134] rank 2 in world size 4 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 2
[1;36m(VllmWorker TP0 pid=1773795)[0;0m WARNING 12-19 17:40:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker TP1 pid=1773798)[0;0m WARNING 12-19 17:40:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker TP1 pid=1773797)[0;0m WARNING 12-19 17:40:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker TP0 pid=1773796)[0;0m WARNING 12-19 17:40:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:30 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:30 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:30 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:30 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:30 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:31 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:31 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:31 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:31 [weight_utils.py:294] Using model weights format ['*.safetensors']
 |‚ñà‚ñà‚ñé       | 02:20 elapsed, 07:39 remaining |‚ñà‚ñà‚ñé       | 02:20 elapsed, 07:39 remaining[1;36m(VllmWorker TP0 pid=1773795)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
 |‚ñà‚ñà‚ñç       | 02:25 elapsed, 07:34 remaining |‚ñà‚ñà‚ñç       | 02:25 elapsed, 07:34 remaining[1;36m(VllmWorker TP0 pid=1773795)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.17s/it]
 |‚ñà‚ñà‚ñå       | 02:30 elapsed, 07:29 remaining |‚ñà‚ñà‚ñå       | 02:30 elapsed, 07:29 remaining[1;36m(VllmWorker TP0 pid=1773795)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:11,  5.73s/it]
[1;36m(VllmWorker TP0 pid=1773795)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:03,  3.65s/it]
 |‚ñà‚ñà‚ñå       | 02:35 elapsed, 07:24 remaining |‚ñà‚ñà‚ñå       | 02:35 elapsed, 07:24 remaining |‚ñà‚ñà‚ñã       | 02:40 elapsed, 07:19 remaining |‚ñà‚ñà‚ñã       | 02:40 elapsed, 07:19 remaining[1;36m(VllmWorker TP0 pid=1773795)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.81s/it]
[1;36m(VllmWorker TP0 pid=1773795)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.76s/it]
[1;36m(VllmWorker TP0 pid=1773795)[0;0m 
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:51 [default_loader.py:267] Loading weights took 20.09 seconds
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:51 [default_loader.py:267] Loading weights took 19.45 seconds
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:51 [default_loader.py:267] Loading weights took 19.90 seconds
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:51 [default_loader.py:267] Loading weights took 19.30 seconds
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:40:51 [gpu_model_runner.py:1983] Model loading took 7.5123 GiB and 21.046317 seconds
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:40:51 [gpu_model_runner.py:1983] Model loading took 7.5123 GiB and 21.127605 seconds
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:40:52 [gpu_model_runner.py:1983] Model loading took 7.5123 GiB and 21.169201 seconds
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:40:52 [gpu_model_runner.py:1983] Model loading took 7.5123 GiB and 21.237032 seconds
 |‚ñà‚ñà‚ñä       | 02:45 elapsed, 07:14 remaining |‚ñà‚ñà‚ñä       | 02:45 elapsed, 07:14 remaining |‚ñà‚ñà‚ñä       | 02:50 elapsed, 07:09 remaining |‚ñà‚ñà‚ñä       | 02:50 elapsed, 07:09 remaining |‚ñà‚ñà‚ñâ       | 02:55 elapsed, 07:04 remaining |‚ñà‚ñà‚ñâ       | 02:55 elapsed, 07:04 remaining[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:41:08 [gpu_worker.py:276] Available KV cache memory: 25.00 GiB
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:41:08 [gpu_worker.py:276] Available KV cache memory: 25.00 GiB
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:41:08 [gpu_worker.py:276] Available KV cache memory: 25.00 GiB
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:41:08 [gpu_worker.py:276] Available KV cache memory: 25.00 GiB
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:849] GPU KV cache size: 409,648 tokens
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.13x
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:849] GPU KV cache size: 409,648 tokens
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.13x
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:849] GPU KV cache size: 409,648 tokens
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.13x
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:849] GPU KV cache size: 409,648 tokens
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:41:08 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.13x
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:41:08 [core.py:215] init engine (profile, create kv cache, warmup model) took 16.43 seconds
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:41:08 [core.py:215] init engine (profile, create kv cache, warmup model) took 16.51 seconds
INFO 12-19 17:41:09 [coordinator.py:187] All engine subscriptions received by DP coordinator
[1;36m(EngineCore_0 pid=1773298)[0;0m INFO 12-19 17:41:09 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(EngineCore_1 pid=1773299)[0;0m INFO 12-19 17:41:09 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:09 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 51206
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:09 [loggers.py:142] Engine 001: vllm cache_config_info with initialization after num_gpu_blocks is: 51206
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:09 [async_llm.py:151] Torch profiler enabled. AsyncLLM CPU traces will be collected under /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:09 [api_server.py:1679] Supported_tasks: ['generate']
[1;36m(APIServer pid=1772573)[0;0m WARNING 12-19 17:41:09 [__init__.py:1670] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:09 [serving_responses.py:124] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [serving_chat.py:135] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [api_server.py:1950] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:36] Available routes are:
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /start_profile, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /stop_profile, Methods: POST
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:10 [launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1772573)[0;0m INFO:     Started server process [1772573]
[1;36m(APIServer pid=1772573)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=1772573)[0;0m INFO:     Application startup complete.
 |‚ñà‚ñà‚ñà       | 03:00 elapsed, 06:59 remaining |‚ñà‚ñà‚ñà       | 03:00 elapsed, 06:59 remaining[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:55460 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(VllmWorker TP1 pid=1773797)[0;0m WARNING 12-19 17:41:11 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(VllmWorker TP0 pid=1773795)[0;0m WARNING 12-19 17:41:11 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:20 [loggers.py:123] Engine 000: Avg prompt throughput: 184.9 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
 |‚ñà‚ñà‚ñà       | 03:10 elapsed, 07:23 remaining
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:21 [api_server.py:1258] Starting profiler...
ERROR: External init callback must run in same thread as registerClient (1298048768 != 1576381312)
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:21 [api_server.py:1260] Profiler started.
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:55460 - "POST /start_profile HTTP/1.1" 200 OK
Initial test run completed. Starting main benchmark run...
Starting profiler...
Traffic request rate: 10.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
  0%|          | 0/50 [00:00<?, ?it/s][1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:55460 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48524 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48534 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48548 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48562 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(VllmWorker TP1 pid=1773798)[0;0m WARNING 12-19 17:41:22 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(VllmWorker TP0 pid=1773796)[0;0m WARNING 12-19 17:41:22 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48576 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48588 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48598 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48604 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48614 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48628 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48636 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48640 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48642 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48648 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48658 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48664 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48666 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48668 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48676 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48682 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48690 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48704 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48708 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48720 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48728 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48738 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48752 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48762 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48766 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48770 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48772 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48774 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48780 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48788 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48798 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48804 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48806 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48814 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48816 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48824 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48832 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48846 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48854 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48860 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48870 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48872 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48876 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48882 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:48898 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:30 [loggers.py:123] Engine 000: Avg prompt throughput: 5324.2 tokens/s, Avg generation throughput: 482.9 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.2%, Prefix cache hit rate: 3.7%
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:30 [loggers.py:123] Engine 001: Avg prompt throughput: 4914.5 tokens/s, Avg generation throughput: 448.1 tokens/s, Running: 24 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.1%, Prefix cache hit rate: 0.0%
  2%|‚ñè         | 1/50 [00:15<12:56, 15.85s/it]  4%|‚ñç         | 2/50 [00:15<05:17,  6.61s/it]  8%|‚ñä         | 4/50 [00:16<01:55,  2.50s/it] 12%|‚ñà‚ñè        | 6/50 [00:16<00:59,  1.35s/it] 16%|‚ñà‚ñå        | 8/50 [00:16<00:34,  1.21it/s] 20%|‚ñà‚ñà        | 10/50 [00:16<00:22,  1.77it/s] 24%|‚ñà‚ñà‚ñç       | 12/50 [00:16<00:15,  2.42it/s] 28%|‚ñà‚ñà‚ñä       | 14/50 [00:17<00:11,  3.15it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:17<00:07,  4.28it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:17<00:05,  5.63it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:17<00:04,  6.22it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:18<00:05,  5.33it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:18<00:04,  6.40it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:18<00:03,  7.96it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:18<00:02,  7.93it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:18<00:02,  9.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:18<00:01, 10.23it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:18<00:01, 11.74it/s][1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:40 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 832.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 3.7%
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:40 [loggers.py:123] Engine 001: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 767.4 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:19<00:01, 13.10it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:19<00:00, 14.28it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:19<00:00, 10.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:19<00:00, 11.63it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:19<00:00, 12.93it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:19<00:00, 14.19it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:19<00:00, 15.19it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:20<00:00, 12.68it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:20<00:00,  2.48it/s]
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:42 [api_server.py:1265] Stopping profiler...
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:50 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 3.7%
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:41:50 [loggers.py:123] Engine 001: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:42:00 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 3.7%
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:42:00 [loggers.py:123] Engine 001: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:46:00 [api_server.py:1267] Profiler stopped.
[1;36m(APIServer pid=1772573)[0;0m INFO:     127.0.0.1:55460 - "POST /stop_profile HTTP/1.1" 200 OK
============ Serving Benchmark Result ============
Successful requests:                     50        
Request rate configured (RPS):           10.00     
Benchmark duration (s):                  20.15     
Total input tokens:                      102350    
Total generated tokens:                  25600     
Request throughput (req/s):              2.48      
Output token throughput (tok/s):         1270.66   
Total Token throughput (tok/s):          6350.83   
---------------Time to First Token----------------
Mean TTFT (ms):                          341.12    
Median TTFT (ms):                        237.73    
P99 TTFT (ms):                           966.23    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          29.23     
Median TPOT (ms):                        29.38     
P99 TPOT (ms):                           30.07     
---------------Inter-token Latency----------------
Mean ITL (ms):                           29.23     
Median ITL (ms):                         27.66     
P99 ITL (ms):                            37.15     
==================================================
Stopping profiler...
Stopping server for llama3_4gpu_tp_2_dp2
[1;36m(APIServer pid=1772573)[0;0m INFO 12-19 17:46:00 [launcher.py:101] Shutting down FastAPI HTTP server.
[1;36m(VllmWorker TP0 pid=1773795)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                                aten::mm        13.09%        1.757s        18.81%        2.523s      28.810us        5.871s        56.34%        5.872s      67.050us         87579  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.695s        25.86%        2.695s      77.984us         34560  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.50%     470.055ms        17.55%        2.354s     110.269us        2.170s        20.82%        2.360s     110.550us         21344  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.494s        14.33%        1.494s      84.400us         17696  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.446s        13.87%        1.446s     220.379us          6560  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                _C_custom_ar::all_reduce         1.46%     195.477ms         7.03%     943.523ms      21.378us        1.423s        13.65%        1.424s      32.268us         44135  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        1.298s        12.46%        1.298s      29.413us         44135  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     904.365ms         8.68%     904.365ms      24.769us         36512  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     541.857ms         5.20%     541.857ms     148.535us          3648  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                             aten::copy_         3.01%     403.683ms         6.02%     806.920ms      12.985us     232.514ms         2.23%     235.666ms       3.792us         62144  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     211.035ms         2.03%     211.035ms      56.852us          3712  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                  _C::fused_add_rms_norm         1.18%     157.887ms         3.15%     422.311ms       9.718us     206.834ms         1.98%     207.346ms       4.771us         43456  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     206.834ms         1.98%     206.834ms       4.760us         43456  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     197.249ms         1.89%     197.249ms      58.705us          3360  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     185.914ms         1.78%     185.914ms     397.252us           468  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                        _C::silu_and_mul         0.53%      70.974ms         1.62%     217.348ms      10.003us     182.234ms         1.75%     182.274ms       8.389us         21728  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     182.234ms         1.75%     182.234ms       8.387us         21728  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     152.001ms         1.46%     152.001ms       4.254us         35733  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     124.971ms         1.20%     124.971ms       2.821us         44295  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                    _C::rotary_embedding         0.77%     103.828ms         1.89%     252.964ms      11.642us      98.730ms         0.95%      98.754ms       4.545us         21728  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      98.730ms         0.95%      98.730ms       4.544us         21728  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.82%     109.537ms         1.98%     264.939ms      12.413us      87.731ms         0.84%      88.075ms       4.126us         21344  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      87.731ms         0.84%      87.731ms       4.110us         21344  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      87.191ms         0.84%      87.191ms       7.305us         11936  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      84.965ms         0.82%      84.965ms       4.023us         21120  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us      75.018ms         0.72%      75.018ms     388.696us           193  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                      record_param_comms         0.24%      32.848ms         0.34%      45.886ms      34.398us      59.993ms         0.58%      59.993ms      44.972us          1334  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsSto...         0.00%       0.000us         0.00%       0.000us       0.000us      59.993ms         0.58%      59.993ms      89.944us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us         0.00%       0.000us       0.000us      59.993ms         0.58%      59.993ms      89.944us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.466ms         0.42%      43.466ms       2.479us         17536  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                             aten::fill_         1.11%     149.323ms         2.13%     285.367ms      12.367us      42.296ms         0.41%      42.297ms       1.833us         23074  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      41.432ms         0.40%      41.432ms       1.907us         21728  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      29.832ms         0.29%      29.832ms      23.904us          1248  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                            aten::argmax         0.07%       9.284ms         0.10%      13.610ms      20.404us      27.213ms         0.26%      27.213ms      40.799us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      27.213ms         0.26%      27.213ms      40.799us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      22.467ms         0.22%      22.467ms       5.614us          4002  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      19.287ms         0.19%      19.287ms       8.371us          2304  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      14.590ms         0.14%      14.590ms       7.124us          2048  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      12.472ms         0.12%      12.472ms      22.927us           544  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       9.420ms         0.09%       9.420ms      98.127us            96  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       8.369ms         0.08%       8.369ms      12.547us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       7.101ms         0.07%       7.101ms       7.925us           896  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.969ms         0.06%       5.969ms      11.659us           512  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us       5.840ms         0.06%       5.840ms      22.814us           256  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       5.828ms         0.06%       5.828ms       2.845us          2048  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                      aten::index_select         0.09%      11.949ms         0.19%      25.000ms      18.574us       5.667ms         0.05%       5.667ms       4.210us          1346  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.764ms         0.05%       4.764ms       1.353us          3521  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       4.557ms         0.04%       4.557ms       8.501us           536  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                             aten::index         0.09%      11.524ms         0.25%      34.127ms      50.261us       4.526ms         0.04%       6.205ms       9.139us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.526ms         0.04%       4.526ms       6.666us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                            _C::rms_norm         0.02%       2.650ms         0.07%       9.014ms      13.276us       4.191ms         0.04%       4.191ms       6.172us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void vllm::rms_norm_kernel<c10::BFloat16>(c10::BFloa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.191ms         0.04%       4.191ms       6.172us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.367ms         0.03%       3.367ms       2.524us          1334  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.334ms         0.03%       3.334ms      47.636us            70  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                      aten::masked_fill_         0.06%       7.829ms         0.10%      13.994ms      20.609us       3.256ms         0.03%       3.256ms       4.796us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       3.256ms         0.03%       3.256ms       4.796us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                          cudaEventQuery         0.02%       2.991ms         0.02%       3.046ms       2.253us       3.205ms         0.03%       3.205ms       2.371us          1352  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.01%     804.453us         0.01%     808.240us       0.590us       3.074ms         0.03%       3.074ms       2.244us          1370  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                            Unrecognized         0.42%      56.189ms         0.42%      56.189ms     952.363us       2.029ms         0.02%       2.029ms      34.391us            59  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.595ms         0.02%       1.595ms       2.392us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                         cudaEventRecord         0.03%       4.266ms         0.03%       4.266ms       1.066us       1.575ms         0.02%       1.575ms       0.394us          4002  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                    cudaEventSynchronize         6.85%     918.638ms         6.85%     918.965ms       1.378ms       1.476ms         0.01%       2.260ms       3.388us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.06%       8.490ms         0.13%      16.910ms      25.543us       1.307ms         0.01%       1.307ms       1.974us           662  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.00%       0.000us         0.00%       0.000us       0.000us       1.307ms         0.01%       1.307ms       1.974us           662  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       1.110ms         0.01%       1.110ms       7.761us           143  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                               aten::sub         0.10%      12.942ms         0.16%      21.953ms      32.668us       1.047ms         0.01%       1.050ms       1.563us           672  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.047ms         0.01%       1.047ms       1.569us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                          Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     971.630us         0.01%     971.630us       1.457us           667  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     725.090us         0.01%     725.090us       1.530us           474  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                            Buffer Flush         0.04%       4.989ms         0.04%       5.175ms      82.136us     600.223us         0.01%     600.223us       9.527us            63  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                   Lazy Function Loading         0.01%     719.582us         0.01%     719.582us      89.948us     306.528us         0.00%     306.528us      38.316us             8  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     138.171us         0.00%     138.171us       1.502us            92  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                   cudaDeviceSynchronize         0.00%     312.004us         0.00%     312.004us     156.002us      55.424us         0.00%      55.424us      27.712us             2  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%     198.736us         0.00%     436.065us      25.651us      33.632us         0.00%      33.632us       1.978us            17  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%       0.000us         0.00%       0.000us       0.000us      33.632us         0.00%      33.632us       1.978us            17  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%     131.775us         0.00%     228.814us      13.460us      28.033us         0.00%      28.033us       1.649us            17  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%       0.000us         0.00%       0.000us       0.000us      28.033us         0.00%      28.033us       1.649us            17  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                        Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      12.448us         0.00%      12.448us       1.037us            12  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                   cudaStreamIsCapturing         0.21%      28.408ms         0.21%      28.408ms       0.634us       8.064us         0.00%       8.064us       0.000us         44813  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                             aten::slice         0.59%      79.183ms         0.80%     107.534ms       1.869us       0.000us         0.00%      14.528us       0.000us         57542  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                        aten::as_strided         1.91%     256.609ms         1.91%     256.640ms       0.685us       0.000us         0.00%       0.000us       0.000us        374823  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                             aten::empty         5.18%     694.797ms         5.21%     699.188ms       4.369us       0.000us         0.00%      62.751us       0.000us        160029  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                                aten::to         0.03%       4.094ms         0.51%      68.589ms      12.709us       0.000us         0.00%      13.344ms       2.472us          5397  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                        aten::lift_fresh         0.03%       4.562ms         0.04%       5.056ms       3.653us       0.000us         0.00%       0.000us       0.000us          1384  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                           aten::detach_         0.03%       3.375ms         0.03%       3.375ms       4.877us       0.000us         0.00%       0.000us       0.000us           692  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                        c10d::allreduce_         0.06%       7.577ms         0.06%       7.577ms      11.159us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                         gloo:all_reduce         0.00%       0.000us             0     781.092ms       1.150ms       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                               aten::max         0.02%       2.639ms         0.03%       4.536ms       6.681us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                            aten::cumsum         0.03%       3.799ms         0.08%      10.416ms      15.341us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                          aten::_to_copy         0.07%       9.914ms         0.48%      64.494ms      19.200us       0.000us         0.00%      13.344ms       3.973us          3359  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                     aten::empty_strided         3.77%     506.190ms         3.83%     513.182ms       5.880us       0.000us         0.00%       3.840us       0.000us         87277  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                TorchDynamo Cache Lookup         0.12%      15.899ms         0.12%      15.903ms      23.421us       0.000us         0.00%     112.225us       0.165us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                              Torch-Compiled Region: 0/1         0.01%       1.359ms         0.02%       2.737ms     160.973us       0.000us         0.00%      61.665us       3.627us            17  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                          cuLaunchKernel         0.14%      18.854ms         0.14%      18.864ms       8.575us       0.000us         0.00%       0.000us       0.000us          2200  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                         aten::embedding         0.01%       1.348ms         0.19%      24.977ms      36.785us       0.000us         0.00%       5.667ms       8.345us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                           aten::resize_         0.02%       3.105ms         0.02%       3.107ms       4.576us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                        cudaLaunchKernel        17.05%        2.287s        17.18%        2.305s       6.018us       0.000us         0.00%       2.486ms       0.006us        382955  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                         aten::unsqueeze         0.02%       2.152ms         0.03%       3.542ms       2.631us       0.000us         0.00%       0.000us       0.000us          1346  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                            aten::expand         0.67%      90.117ms         0.86%     115.272ms       1.763us       0.000us         0.00%       1.696us       0.000us         65378  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m                                        vllm::all_reduce         6.69%     897.064ms        16.25%        2.180s      49.401us       0.000us         0.00%        1.425s      32.276us         44135  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP0 pid=1773795)[0;0m Self CPU time total: 13.414s
[1;36m(VllmWorker TP0 pid=1773795)[0;0m Self CUDA time total: 10.421s
[1;36m(VllmWorker TP0 pid=1773795)[0;0m 
[1;36m(VllmWorker TP0 pid=1773795)[0;0m INFO 12-19 17:46:00 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker TP0 pid=1773796)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                                aten::mm        13.44%        1.746s        19.25%        2.500s      28.547us        5.789s        56.03%        5.791s      66.131us         87569  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.697s        26.10%        2.697s      77.379us         34848  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.64%     472.448ms        18.06%        2.345s     111.559us        2.018s        19.53%        2.208s     105.018us         21024  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                _C_custom_ar::all_reduce         1.52%     197.812ms         7.28%     945.569ms      21.424us        1.579s        15.28%        1.579s      35.778us         44135  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        1.455s        14.08%        1.455s      32.975us         44135  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.421s        13.76%        1.421s      80.462us         17664  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.374s        13.30%        1.374s     220.227us          6240  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     922.341ms         8.93%     922.341ms      24.912us         37024  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     463.504ms         4.49%     463.504ms     137.948us          3360  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                             aten::copy_         3.09%     401.515ms         6.17%     800.627ms      12.919us     230.309ms         2.23%     233.124ms       3.762us         61971  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                  _C::fused_add_rms_norm         1.23%     160.178ms         3.21%     417.071ms       9.598us     205.848ms         1.99%     205.877ms       4.738us         43456  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     205.848ms         1.99%     205.848ms       4.737us         43456  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     191.888ms         1.86%     191.888ms      60.571us          3168  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     187.436ms         1.81%     187.436ms      56.321us          3328  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                        _C::silu_and_mul         0.54%      70.041ms         1.64%     212.680ms       9.788us     180.818ms         1.75%     180.973ms       8.329us         21728  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     180.818ms         1.75%     180.818ms       8.322us         21728  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     177.757ms         1.72%     177.757ms     395.895us           449  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     151.347ms         1.46%     151.347ms       4.237us         35721  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     123.403ms         1.19%     123.403ms       2.788us         44263  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                    _C::rotary_embedding         0.79%     102.006ms         1.91%     248.602ms      11.442us      97.646ms         0.95%      97.652ms       4.494us         21728  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      97.646ms         0.95%      97.646ms       4.494us         21728  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      88.373ms         0.86%      88.373ms       7.306us         12096  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      86.261ms         0.83%      86.261ms       3.982us         21664  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.82%     105.854ms         1.99%     258.320ms      12.287us      86.151ms         0.83%      86.332ms       4.106us         21024  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      86.151ms         0.83%      86.151ms       4.098us         21024  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us      77.858ms         0.75%      77.858ms     389.289us           200  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                      record_param_comms         0.25%      32.088ms         0.35%      45.019ms      34.261us      58.021ms         0.56%      58.021ms      44.156us          1314  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsSto...         0.00%       0.000us         0.00%       0.000us       0.000us      58.021ms         0.56%      58.021ms      88.313us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us         0.00%       0.000us       0.000us      58.021ms         0.56%      58.021ms      88.313us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.766ms         0.42%      43.766ms       2.496us         17536  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                             aten::fill_         1.12%     145.082ms         2.13%     276.246ms      11.977us      41.997ms         0.41%      42.002ms       1.821us         23064  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      41.148ms         0.40%      41.148ms       1.894us         21728  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                            aten::argmax         0.07%       8.890ms         0.10%      13.036ms      19.842us      26.728ms         0.26%      26.728ms      40.682us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      26.728ms         0.26%      26.728ms      40.682us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      21.101ms         0.20%      21.101ms       5.353us          3942  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      19.978ms         0.19%      19.978ms      24.012us           832  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      19.106ms         0.18%      19.106ms      22.964us           832  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      16.946ms         0.16%      16.946ms       8.275us          2048  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      16.400ms         0.16%      16.400ms       7.118us          2304  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us       9.507ms         0.09%       9.507ms      22.853us           416  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       9.380ms         0.09%       9.380ms      97.708us            96  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.907ms         0.08%       7.907ms      12.034us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       5.865ms         0.06%       5.865ms       2.820us          2080  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.802ms         0.06%       5.802ms      10.665us           544  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                      aten::index_select         0.08%      10.437ms         0.18%      23.228ms      17.386us       5.697ms         0.06%       5.697ms       4.265us          1336  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.310ms         0.05%       5.310ms       7.903us           672  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       4.681ms         0.05%       4.681ms      45.011us           104  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.614ms         0.04%       4.614ms       1.370us          3368  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                             aten::index         0.09%      11.067ms         0.26%      33.431ms      49.235us       4.499ms         0.04%       6.253ms       9.209us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.499ms         0.04%       4.499ms       6.626us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       4.402ms         0.04%       4.402ms       8.497us           518  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                            _C::rms_norm         0.02%       2.650ms         0.07%       8.935ms      13.159us       4.172ms         0.04%       4.225ms       6.222us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void vllm::rms_norm_kernel<c10::BFloat16>(c10::BFloa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.172ms         0.04%       4.172ms       6.144us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                          cudaEventQuery         0.02%       3.039ms         0.02%       3.134ms       2.351us       3.467ms         0.03%       3.471ms       2.604us          1333  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.444ms         0.03%       3.444ms       2.621us          1314  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                      aten::masked_fill_         0.06%       7.850ms         0.11%      13.961ms      20.562us       3.252ms         0.03%       3.252ms       4.789us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       3.252ms         0.03%       3.252ms       4.789us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.01%     790.521us         0.01%     795.572us       0.588us       2.315ms         0.02%       2.315ms       1.712us          1352  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                            Unrecognized         0.43%      55.320ms         0.43%      55.320ms     937.619us       2.032ms         0.02%       2.032ms      34.448us            59  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.772ms         0.02%       1.772ms      55.361us            32  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                         cudaEventRecord         0.03%       4.225ms         0.03%       4.225ms       1.072us       1.575ms         0.02%       1.575ms       0.400us          3942  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.520ms         0.01%       1.520ms       2.314us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                    cudaEventSynchronize         6.85%     889.353ms         6.85%     889.722ms       1.354ms       1.435ms         0.01%       3.263ms       4.967us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       1.296ms         0.01%       1.296ms       8.049us           161  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.07%       8.456ms         0.13%      16.766ms      25.675us       1.289ms         0.01%       1.289ms       1.974us           653  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.00%       0.000us         0.00%       0.000us       0.000us       1.289ms         0.01%       1.289ms       1.974us           653  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                            Buffer Flush         0.04%       5.342ms         0.04%       5.501ms      87.312us       1.100ms         0.01%       1.100ms      17.463us            63  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                               aten::sub         0.07%       9.522ms         0.13%      17.488ms      26.577us       1.018ms         0.01%       1.020ms       1.550us           658  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.018ms         0.01%       1.018ms       1.550us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                          Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     961.444us         0.01%     961.444us       1.463us           657  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     693.664us         0.01%     693.664us       1.518us           457  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                   Lazy Function Loading         0.01%     955.772us         0.01%     955.772us      63.718us     385.410us         0.00%     385.410us      25.694us            15  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     155.301us         0.00%     155.301us       1.493us           104  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                        Runtime Triggered Module Loading         0.05%       6.997ms         0.05%       6.997ms       1.166ms     121.410us         0.00%     121.410us      20.235us             6  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                   cudaDeviceSynchronize         0.00%     310.371us         0.00%     310.371us     155.185us      65.952us         0.00%      65.952us      32.976us             2  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                   cudaStreamIsCapturing         0.21%      27.172ms         0.21%      27.172ms       0.607us      55.935us         0.00%      55.935us       0.001us         44797  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%     370.955us         0.01%     770.772us      29.645us      51.198us         0.00%      51.198us       1.969us            26  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%       0.000us         0.00%       0.000us       0.000us      51.198us         0.00%      51.198us       1.969us            26  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%     228.345us         0.00%     385.409us      14.823us      42.399us         0.00%      42.399us       1.631us            26  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%       0.000us         0.00%       0.000us       0.000us      42.399us         0.00%      42.399us       1.631us            26  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                        Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      24.253us         0.00%      24.253us       1.102us            22  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                             aten::slice         0.59%      76.700ms         0.80%     104.292ms       1.840us       0.000us         0.00%       0.000us       0.000us         56690  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                        aten::as_strided         1.91%     247.391ms         1.91%     247.429ms       0.665us       0.000us         0.00%      53.375us       0.000us        372225  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                             aten::empty         5.17%     671.232ms         5.22%     677.538ms       4.254us       0.000us         0.00%     111.073us       0.001us        159254  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                                aten::to         0.03%       3.930ms         0.54%      69.752ms      13.052us       0.000us         0.00%      12.909ms       2.416us          5344  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                        aten::lift_fresh         0.00%     279.132us         0.00%     279.132us       0.206us       0.000us         0.00%       0.000us       0.000us          1358  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                           aten::detach_         0.00%      94.533us         0.00%      94.533us       0.139us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                        c10d::allreduce_         0.06%       7.344ms         0.06%       7.352ms      10.828us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                         gloo:all_reduce         0.00%       0.000us             0        1.245s       1.834ms       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                               aten::max         0.02%       2.637ms         0.03%       4.450ms       6.554us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                            aten::cumsum         0.04%       4.570ms         0.10%      13.486ms      19.861us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                          aten::_to_copy         0.08%       9.955ms         0.51%      65.822ms      19.772us       0.000us         0.00%      12.909ms       3.878us          3329  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                     aten::empty_strided         3.85%     499.406ms         3.87%     502.895ms       5.791us       0.000us         0.00%      64.736us       0.001us         86841  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                TorchDynamo Cache Lookup         0.13%      16.500ms         0.13%      16.500ms      24.300us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                              Torch-Compiled Region: 0/1         0.02%       2.383ms         0.04%       4.850ms     186.538us       0.000us         0.00%      93.597us       3.600us            26  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                          cuLaunchKernel         0.13%      16.780ms         0.13%      16.780ms       8.592us       0.000us         0.00%       0.000us       0.000us          1953  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                         aten::embedding         0.01%       1.307ms         0.18%      23.177ms      34.135us       0.000us         0.00%       5.697ms       8.391us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                           aten::resize_         0.02%       2.996ms         0.02%       2.996ms       4.412us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                        cudaLaunchKernel        17.37%        2.256s        17.54%        2.277s       5.945us       0.000us         0.00%       1.539ms       0.004us        383028  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m                                         aten::unsqueeze         0.02%       2.104ms         0.03%       3.446ms       2.580us       0.000us         0.00%       0.000us       0.000us          1336  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP0 pid=1773796)[0;0m Self CPU time total: 12.984s
[1;36m(VllmWorker TP0 pid=1773796)[0;0m Self CUDA time total: 10.333s
[1;36m(VllmWorker TP0 pid=1773796)[0;0m 
[1;36m(VllmWorker TP0 pid=1773796)[0;0m INFO 12-19 17:46:00 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker TP1 pid=1773798)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                                aten::mm        13.21%        1.722s        18.88%        2.461s      28.101us        5.788s        43.56%        5.789s      66.112us         87569  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                _C_custom_ar::all_reduce         1.46%     190.914ms         7.11%     926.285ms      20.988us        4.488s        33.78%        4.489s     101.709us         44135  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        4.365s        32.85%        4.365s      98.904us         44135  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.694s        20.28%        2.694s      77.321us         34848  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.58%     466.107ms        17.78%        2.317s     110.203us        2.017s        15.18%        2.215s     105.355us         21024  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.422s        10.71%        1.422s      80.521us         17664  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.374s        10.34%        1.374s     220.201us          6240  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     922.907ms         6.95%     922.907ms      24.927us         37024  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     463.587ms         3.49%     463.587ms     137.972us          3360  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                             aten::copy_         3.04%     395.964ms         6.05%     788.063ms      12.717us     238.767ms         1.80%     241.936ms       3.904us         61971  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                  _C::fused_add_rms_norm         1.12%     145.985ms         3.15%     410.095ms       9.437us     207.293ms         1.56%     207.411ms       4.773us         43456  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     207.293ms         1.56%     207.293ms       4.770us         43456  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     191.854ms         1.44%     191.854ms      60.560us          3168  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     187.491ms         1.41%     187.491ms      56.337us          3328  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                        _C::silu_and_mul         0.53%      68.838ms         1.61%     209.364ms       9.636us     180.424ms         1.36%     180.541ms       8.309us         21728  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     180.424ms         1.36%     180.424ms       8.304us         21728  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     177.818ms         1.34%     177.818ms     396.032us           449  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     151.555ms         1.14%     151.555ms       4.243us         35721  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     123.411ms         0.93%     123.411ms       2.788us         44263  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                    _C::rotary_embedding         0.72%      94.078ms         1.85%     240.604ms      11.073us      97.122ms         0.73%      97.204ms       4.474us         21728  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      97.122ms         0.73%      97.122ms       4.470us         21728  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                      record_param_comms         0.24%      31.371ms         0.34%      44.823ms      34.112us      95.099ms         0.72%      95.128ms      72.395us          1314  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsSto...         0.00%       0.000us         0.00%       0.000us       0.000us      95.099ms         0.72%      95.099ms     144.747us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us         0.00%       0.000us       0.000us      95.099ms         0.72%      95.099ms     144.747us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      86.823ms         0.65%      86.823ms       7.178us         12096  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      86.278ms         0.65%      86.278ms       3.983us         21664  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.79%     102.593ms         1.97%     256.403ms      12.196us      85.504ms         0.64%      85.741ms       4.078us         21024  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      85.504ms         0.64%      85.504ms       4.067us         21024  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us      77.807ms         0.59%      77.807ms     389.037us           200  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      51.760ms         0.39%      51.760ms       2.952us         17536  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                             aten::fill_         1.11%     144.737ms         2.11%     275.134ms      11.929us      42.679ms         0.32%      42.864ms       1.858us         23064  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      41.698ms         0.31%      41.698ms       1.919us         21728  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                            aten::argmax         0.07%       8.887ms         0.10%      12.966ms      19.735us      26.748ms         0.20%      26.756ms      40.725us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      26.748ms         0.20%      26.748ms      40.712us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      20.863ms         0.16%      20.863ms       5.292us          3942  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      19.982ms         0.15%      19.982ms      24.017us           832  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      19.053ms         0.14%      19.053ms      22.900us           832  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      16.947ms         0.13%      16.947ms       8.275us          2048  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      16.255ms         0.12%      16.255ms       7.055us          2304  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us       9.532ms         0.07%       9.532ms      22.914us           416  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       9.380ms         0.07%       9.380ms      97.708us            96  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.936ms         0.06%       7.936ms      12.079us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       5.881ms         0.04%       5.881ms       2.828us          2080  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.827ms         0.04%       5.827ms      10.712us           544  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.318ms         0.04%       5.318ms       7.914us           672  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                      aten::index_select         0.08%      10.285ms         0.18%      23.093ms      17.285us       5.254ms         0.04%       5.254ms       3.932us          1336  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                          cudaEventQuery         0.02%       2.560ms         0.02%       2.695ms       2.017us       4.836ms         0.04%       4.890ms       3.660us          1336  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       4.680ms         0.04%       4.680ms      44.997us           104  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.548ms         0.03%       4.548ms       1.350us          3368  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.00%     514.005us         0.00%     514.697us       0.379us       4.517ms         0.03%       4.517ms       3.326us          1358  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                             aten::index         0.08%      10.945ms         0.25%      32.805ms      48.314us       4.496ms         0.03%       6.462ms       9.517us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.496ms         0.03%       4.496ms       6.622us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       4.272ms         0.03%       4.272ms       8.248us           518  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                            _C::rms_norm         0.02%       2.472ms         0.06%       8.467ms      12.469us       4.153ms         0.03%       4.153ms       6.116us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void vllm::rms_norm_kernel<c10::BFloat16>(c10::BFloa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.153ms         0.03%       4.153ms       6.116us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.767ms         0.03%       3.767ms       2.866us          1314  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                      aten::masked_fill_         0.06%       7.674ms         0.10%      13.603ms      20.034us       3.109ms         0.02%       3.109ms       4.579us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       3.109ms         0.02%       3.109ms       4.579us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.769ms         0.01%       1.769ms      55.295us            32  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                         cudaEventRecord         0.03%       4.068ms         0.03%       4.068ms       1.032us       1.766ms         0.01%       1.766ms       0.448us          3942  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.679ms         0.01%       1.679ms       2.555us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                            Unrecognized         0.43%      56.614ms         0.43%      56.614ms     959.556us       1.648ms         0.01%       1.648ms      27.940us            59  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                    cudaEventSynchronize         8.33%        1.086s         8.33%        1.086s       1.653ms       1.640ms         0.01%       3.358ms       5.111us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.07%       8.589ms         0.13%      16.849ms      25.803us       1.346ms         0.01%       1.346ms       2.061us           653  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.00%       0.000us         0.00%       0.000us       0.000us       1.346ms         0.01%       1.346ms       2.061us           653  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                               aten::sub         0.07%       9.413ms         0.13%      17.221ms      26.172us       1.322ms         0.01%       1.324ms       2.012us           658  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.322ms         0.01%       1.322ms       2.012us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us     981.567us         0.01%     981.567us       6.097us           161  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                          Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     946.248us         0.01%     946.248us       1.440us           657  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                            Buffer Flush         0.03%       4.560ms         0.04%       4.717ms      74.877us     909.508us         0.01%     909.508us      14.437us            63  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     802.264us         0.01%     802.264us       1.756us           457  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                   Lazy Function Loading         0.01%     996.274us         0.01%     996.274us      66.418us     388.480us         0.00%     388.480us      25.899us            15  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     178.721us         0.00%     178.721us       1.718us           104  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                        Runtime Triggered Module Loading         0.06%       7.279ms         0.06%       7.279ms       1.213ms     121.922us         0.00%     121.922us      20.320us             6  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                   cudaDeviceSynchronize         0.00%     303.358us         0.00%     303.358us     151.679us      96.991us         0.00%      96.991us      48.495us             2  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%     359.852us         0.01%     747.259us      28.741us      55.455us         0.00%      55.455us       2.133us            26  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%       0.000us         0.00%       0.000us       0.000us      55.455us         0.00%      55.455us       2.133us            26  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%     235.354us         0.00%     396.519us      15.251us      49.697us         0.00%      49.697us       1.911us            26  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%       0.000us         0.00%       0.000us       0.000us      49.697us         0.00%      49.697us       1.911us            26  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                        Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      23.040us         0.00%      23.040us       1.047us            22  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                   cudaStreamIsCapturing         0.22%      28.994ms         0.22%      28.994ms       0.647us      22.176us         0.00%      22.176us       0.000us         44797  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                             aten::slice         0.58%      75.529ms         0.80%     103.734ms       1.830us       0.000us         0.00%       0.000us       0.000us         56690  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                        aten::as_strided         1.86%     242.161ms         1.86%     242.193ms       0.651us       0.000us         0.00%       0.000us       0.000us        372225  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                             aten::empty         5.17%     673.502ms         5.23%     682.116ms       4.283us       0.000us         0.00%      34.912us       0.000us        159254  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                                aten::to         0.03%       4.006ms         0.54%      70.863ms      13.260us       0.000us         0.00%      13.881ms       2.598us          5344  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                        aten::lift_fresh         0.00%     270.837us         0.00%     270.837us       0.199us       0.000us         0.00%       0.000us       0.000us          1358  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                           aten::detach_         0.00%      98.991us         0.00%      98.991us       0.146us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                        c10d::allreduce_         0.06%       7.189ms         0.06%       7.189ms      10.587us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                         gloo:all_reduce         0.00%       0.000us             0        1.250s       1.841ms       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                               aten::max         0.02%       2.828ms         0.04%       4.806ms       7.079us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                            aten::cumsum         0.04%       5.085ms         0.11%      14.868ms      21.896us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                          aten::_to_copy         0.08%      10.055ms         0.51%      66.857ms      20.083us       0.000us         0.00%      13.881ms       4.170us          3329  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                     aten::empty_strided         3.78%     492.374ms         3.79%     494.578ms       5.695us       0.000us         0.00%     245.920us       0.003us         86841  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                TorchDynamo Cache Lookup         0.13%      17.363ms         0.13%      17.363ms      25.571us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                              Torch-Compiled Region: 0/1         0.02%       2.393ms         0.04%       4.680ms     179.991us       0.000us         0.00%     105.152us       4.044us            26  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                          cuLaunchKernel         0.13%      16.712ms         0.13%      16.712ms       8.557us       0.000us         0.00%       0.000us       0.000us          1953  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                         aten::embedding         0.01%       1.281ms         0.18%      22.996ms      33.868us       0.000us         0.00%       5.254ms       7.738us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                           aten::resize_         0.02%       3.110ms         0.02%       3.110ms       4.580us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                        cudaLaunchKernel        17.05%        2.223s        17.22%        2.245s       5.861us       0.000us         0.00%       1.401ms       0.004us        383028  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m                                         aten::unsqueeze         0.02%       2.112ms         0.03%       3.402ms       2.547us       0.000us         0.00%       0.000us       0.000us          1336  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP1 pid=1773798)[0;0m Self CPU time total: 13.034s
[1;36m(VllmWorker TP1 pid=1773798)[0;0m Self CUDA time total: 13.286s
[1;36m(VllmWorker TP1 pid=1773798)[0;0m 
[1;36m(VllmWorker TP1 pid=1773798)[0;0m INFO 12-19 17:46:00 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker TP1 pid=1773797)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                _C_custom_ar::all_reduce         1.41%     193.287ms         6.72%     919.709ms      20.839us        6.461s        41.68%        6.462s     146.412us         44135  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        6.336s        40.88%        6.336s     143.565us         44135  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                                aten::mm        12.47%        1.707s        17.86%        2.444s      27.907us        5.865s        37.84%        5.868s      67.000us         87579  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.695s        17.38%        2.695s      77.967us         34560  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.40%     464.730ms        16.91%        2.314s     108.406us        2.167s        13.98%        2.357s     110.415us         21344  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.492s         9.62%        1.492s      84.311us         17696  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.444s         9.32%        1.444s     220.146us          6560  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     901.346ms         5.81%     901.346ms      24.686us         36512  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     541.461ms         3.49%     541.461ms     148.427us          3648  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                             aten::copy_         2.95%     404.001ms         5.84%     799.807ms      12.870us     232.644ms         1.50%     236.418ms       3.804us         62144  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     210.585ms         1.36%     210.585ms      56.731us          3712  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                  _C::fused_add_rms_norm         1.15%     157.473ms         3.02%     413.589ms       9.517us     207.811ms         1.34%     208.013ms       4.787us         43456  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     207.811ms         1.34%     207.811ms       4.782us         43456  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     197.230ms         1.27%     197.230ms      58.699us          3360  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     186.024ms         1.20%     186.024ms     397.488us           468  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                        _C::silu_and_mul         0.50%      68.271ms         1.49%     204.230ms       9.399us     182.308ms         1.18%     182.350ms       8.392us         21728  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     182.308ms         1.18%     182.308ms       8.390us         21728  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     152.507ms         0.98%     152.507ms       4.268us         35733  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     124.943ms         0.81%     124.943ms       2.821us         44295  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                      record_param_comms         0.24%      32.188ms         0.33%      45.116ms      33.820us     106.985ms         0.69%     106.985ms      80.199us          1334  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsSto...         0.00%       0.000us         0.00%       0.000us       0.000us     106.985ms         0.69%     106.985ms     160.398us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us         0.00%       0.000us       0.000us     106.985ms         0.69%     106.985ms     160.398us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                    _C::rotary_embedding         0.76%     103.928ms         1.79%     244.527ms      11.254us     100.264ms         0.65%     100.412ms       4.621us         21728  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us     100.264ms         0.65%     100.264ms       4.614us         21728  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.79%     107.948ms         1.90%     259.929ms      12.178us      88.254ms         0.57%      88.261ms       4.135us         21344  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      88.254ms         0.57%      88.254ms       4.135us         21344  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      86.727ms         0.56%      86.727ms       7.266us         11936  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      84.902ms         0.55%      84.902ms       4.020us         21120  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us      74.986ms         0.48%      74.986ms     388.530us           193  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.242ms         0.28%      43.242ms       2.466us         17536  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                             aten::fill_         1.06%     145.619ms         1.99%     272.243ms      11.799us      42.705ms         0.28%      42.710ms       1.851us         23074  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      41.846ms         0.27%      41.846ms       1.926us         21728  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      29.844ms         0.19%      29.844ms      23.913us          1248  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                            aten::argmax         0.07%       9.008ms         0.10%      14.084ms      21.116us      27.170ms         0.18%      27.211ms      40.796us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      27.170ms         0.18%      27.170ms      40.735us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      22.308ms         0.14%      22.308ms       5.574us          4002  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      19.319ms         0.12%      19.319ms       8.385us          2304  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us      14.545ms         0.09%      14.545ms       7.102us          2048  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      12.482ms         0.08%      12.482ms      22.944us           544  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       9.415ms         0.06%       9.415ms      98.074us            96  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       8.368ms         0.05%       8.368ms      12.545us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       7.062ms         0.05%       7.062ms       7.882us           896  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.992ms         0.04%       5.992ms      11.702us           512  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       5.932ms         0.04%       5.932ms       2.896us          2048  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us       5.835ms         0.04%       5.835ms      22.793us           256  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                      aten::index_select         0.08%      10.266ms         0.17%      22.985ms      17.076us       5.411ms         0.03%       5.411ms       4.020us          1346  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.734ms         0.03%       4.734ms       1.344us          3521  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                             aten::index         0.08%      11.125ms         0.24%      32.968ms      48.554us       4.536ms         0.03%       6.213ms       9.151us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.536ms         0.03%       4.536ms       6.681us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       4.513ms         0.03%       4.513ms       8.419us           536  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                         cudaEventRecord         0.03%       4.161ms         0.03%       4.161ms       1.040us       4.197ms         0.03%       4.197ms       1.049us          4002  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                            _C::rms_norm         0.02%       2.577ms         0.07%       8.912ms      13.125us       4.163ms         0.03%       4.163ms       6.132us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void vllm::rms_norm_kernel<c10::BFloat16>(c10::BFloa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.163ms         0.03%       4.163ms       6.132us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.00%     531.188us         0.00%     531.379us       0.383us       3.819ms         0.02%       3.819ms       2.756us          1386  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                          cudaEventQuery         0.02%       2.985ms         0.02%       3.081ms       2.265us       3.540ms         0.02%       3.591ms       2.640us          1360  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.356ms         0.02%       3.356ms       2.516us          1334  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.351ms         0.02%       3.351ms      47.866us            70  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                      aten::masked_fill_         0.06%       7.770ms         0.10%      13.747ms      20.246us       3.159ms         0.02%       3.159ms       4.653us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       3.159ms         0.02%       3.159ms       4.653us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                    cudaEventSynchronize        10.49%        1.435s        10.49%        1.435s       2.152ms       2.493ms         0.02%       3.898ms       5.844us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                            Unrecognized         0.42%      57.004ms         0.42%      57.004ms     966.176us       2.159ms         0.01%       2.159ms      36.593us            59  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.590ms         0.01%       1.590ms       2.383us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.06%       8.228ms         0.12%      16.450ms      24.849us       1.305ms         0.01%       1.322ms       1.998us           662  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.00%       0.000us         0.00%       0.000us       0.000us       1.305ms         0.01%       1.305ms       1.972us           662  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                            Buffer Flush         0.03%       4.015ms         0.03%       4.131ms      65.577us       1.290ms         0.01%       1.290ms      20.478us            63  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                               aten::sub         0.09%      12.978ms         0.16%      21.922ms      32.622us       1.039ms         0.01%       1.039ms       1.547us           672  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.039ms         0.01%       1.039ms       1.558us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                          Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us     962.290us         0.01%     962.290us       1.443us           667  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us     898.900us         0.01%     898.900us       6.286us           143  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     720.927us         0.00%     720.927us       1.521us           474  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                   Lazy Function Loading         0.01%       1.003ms         0.01%       1.003ms     125.389us     307.993us         0.00%     307.993us      38.499us             8  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     137.718us         0.00%     137.718us       1.497us            92  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                   cudaDeviceSynchronize         0.01%     694.645us         0.01%     694.645us     347.323us      54.975us         0.00%      54.975us      27.488us             2  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%     204.167us         0.00%     444.213us      26.130us      33.471us         0.00%      33.471us       1.969us            17  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%       0.000us         0.00%       0.000us       0.000us      33.471us         0.00%      33.471us       1.969us            17  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%     129.344us         0.00%     226.663us      13.333us      27.745us         0.00%      27.745us       1.632us            17  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%       0.000us         0.00%       0.000us       0.000us      27.745us         0.00%      27.745us       1.632us            17  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                        Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      12.672us         0.00%      12.672us       1.056us            12  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                   cudaStreamIsCapturing         0.21%      28.606ms         0.21%      28.606ms       0.638us       8.096us         0.00%       8.096us       0.000us         44813  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                             aten::slice         0.57%      77.497ms         0.78%     106.576ms       1.852us       0.000us         0.00%       0.000us       0.000us         57542  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                        aten::as_strided         1.83%     250.559ms         1.83%     250.599ms       0.669us       0.000us         0.00%       9.025us       0.000us        374823  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                             aten::empty         4.95%     677.506ms         5.00%     684.327ms       4.276us       0.000us         0.00%      14.720us       0.000us        160029  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                                aten::to         0.03%       3.965ms         0.53%      73.142ms      13.552us       0.000us         0.00%      13.645ms       2.528us          5397  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                        aten::lift_fresh         0.04%       4.876ms         0.04%       5.523ms       3.990us       0.000us         0.00%       0.000us       0.000us          1384  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                           aten::detach_         0.03%       3.511ms         0.03%       3.511ms       5.073us       0.000us         0.00%       0.000us       0.000us           692  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                        c10d::allreduce_         0.06%       7.645ms         0.06%       7.645ms      11.259us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                         gloo:all_reduce         0.00%       0.000us             0     772.595ms       1.138ms       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                               aten::max         0.02%       2.616ms         0.03%       4.526ms       6.665us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                            aten::cumsum         0.03%       3.748ms         0.08%      10.281ms      15.142us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                          aten::_to_copy         0.07%       9.583ms         0.51%      69.176ms      20.594us       0.000us         0.00%      13.645ms       4.062us          3359  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                     aten::empty_strided         3.65%     500.081ms         3.69%     504.583ms       5.781us       0.000us         0.00%     357.755us       0.004us         87277  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                TorchDynamo Cache Lookup         0.12%      16.204ms         0.12%      16.204ms      23.864us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                              Torch-Compiled Region: 0/1         0.01%       1.497ms         0.02%       3.207ms     188.624us       0.000us         0.00%      61.216us       3.601us            17  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                          cuLaunchKernel         0.13%      18.332ms         0.13%      18.341ms       8.337us       0.000us         0.00%      17.216us       0.008us          2200  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                         aten::embedding         0.01%       1.311ms         0.17%      22.922ms      33.759us       0.000us         0.00%       5.411ms       7.970us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                           aten::resize_         0.02%       2.950ms         0.02%       2.950ms       4.345us       0.000us         0.00%       0.000us       0.000us           679  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                        cudaLaunchKernel        16.03%        2.194s        16.16%        2.211s       5.773us       0.000us         0.00%       1.738ms       0.005us        382955  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                         aten::unsqueeze         0.01%       2.044ms         0.02%       3.369ms       2.503us       0.000us         0.00%       0.000us       0.000us          1346  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                            aten::expand         0.67%      91.238ms         0.85%     115.979ms       1.774us       0.000us         0.00%       0.000us       0.000us         65378  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m                                        vllm::all_reduce         6.37%     871.897ms        15.57%        2.131s      48.276us       0.000us         0.00%        6.462s     146.423us         44135  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker TP1 pid=1773797)[0;0m Self CPU time total: 13.685s
[1;36m(VllmWorker TP1 pid=1773797)[0;0m Self CUDA time total: 15.501s
[1;36m(VllmWorker TP1 pid=1773797)[0;0m 
[1;36m(VllmWorker TP1 pid=1773797)[0;0m INFO 12-19 17:46:00 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(APIServer pid=1772573)[0;0m INFO:     Shutting down
[1;36m(APIServer pid=1772573)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=1772573)[0;0m INFO:     Application shutdown complete.
Traces for llama3_4gpu_tp_2_dp2 are in: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_dp2

=== Running experiment: llama3_4gpu_tp_2_pp2 ===
Model=meta-llama/Llama-3.1-8B DP=1 TP=2 PP=2 GPUs=0,1,2,3
Trace dir: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2
Server PID: 1776197
INFO 12-19 17:46:40 [__init__.py:241] Automatically detected platform cuda.
WARNING 12-19 17:46:49 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:46:50 [__init__.py:1750] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:46:50 [api_server.py:1875] vLLM API server version 0.10.1rc2.dev263+g2f13319f4
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:46:50 [utils.py:326] non-default args: {'model': 'meta-llama/Llama-3.1-8B', 'trust_remote_code': True, 'enforce_eager': True, 'disable_sliding_window': True, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'swap_space': 16.0, 'max_num_batched_tokens': 512, 'max_num_seqs': 512, 'enable_chunked_prefill': True}
[1;36m(APIServer pid=1776197)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-19 17:46:58 [__init__.py:241] Automatically detected platform cuda.
WARNING 12-19 17:47:10 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fa40b7ecb80>, seed=0, num_prompts=50, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=2048, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, endpoint_type='openai', label=None, backend='vllm', base_url='http://127.0.0.1:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=10.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=True, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600)
INFO 12-19 17:47:12 [datasets.py:509] Sampling input_len from [2047, 2047] and output_len from [512, 512]
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
 |          | 00:00 elapsed, ? remaining |          | 00:00 elapsed, 00:01 remaining |          | 00:05 elapsed, 09:54 remaining |          | 00:05 elapsed, 09:54 remaining |‚ñè         | 00:10 elapsed, 09:49 remaining |‚ñè         | 00:10 elapsed, 09:49 remaining |‚ñé         | 00:15 elapsed, 09:44 remaining |‚ñé         | 00:15 elapsed, 09:44 remaining[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:47:29 [__init__.py:742] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1776197)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:47:29 [__init__.py:1786] Using max model len 131072
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:47:31 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=512.
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:47:31 [__init__.py:3638] Cudagraph is disabled under eager mode
 |‚ñé         | 00:20 elapsed, 09:39 remaining |‚ñé         | 00:20 elapsed, 09:39 remaining |‚ñç         | 00:25 elapsed, 09:34 remaining |‚ñç         | 00:25 elapsed, 09:34 remaining |‚ñå         | 00:30 elapsed, 09:29 remaining |‚ñå         | 00:30 elapsed, 09:29 remaining |‚ñå         | 00:35 elapsed, 09:24 remaining |‚ñå         | 00:35 elapsed, 09:24 remaining |‚ñã         | 00:40 elapsed, 09:19 remaining |‚ñã         | 00:40 elapsed, 09:19 remaining |‚ñä         | 00:45 elapsed, 09:14 remaining |‚ñä         | 00:45 elapsed, 09:14 remaining |‚ñä         | 00:50 elapsed, 09:09 remaining |‚ñä         | 00:50 elapsed, 09:09 remainingINFO 12-19 17:48:06 [__init__.py:241] Automatically detected platform cuda.
 |‚ñâ         | 00:55 elapsed, 09:04 remaining |‚ñâ         | 00:55 elapsed, 09:04 remaining |‚ñà         | 01:00 elapsed, 08:59 remaining |‚ñà         | 01:00 elapsed, 08:59 remainingWARNING 12-19 17:48:16 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:48:16 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:48:17 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev263+g2f13319f4) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=1776861)[0;0m WARNING 12-19 17:48:17 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:48:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_0e678e3d'), local_subscribe_addr='ipc:///tmp/d05387c6-ac03-4e11-9ed0-9acb298840a5', remote_subscribe_addr=None, remote_addr_ipv6=False)
 |‚ñà         | 01:05 elapsed, 08:54 remaining |‚ñà         | 01:05 elapsed, 08:54 remaining |‚ñà‚ñè        | 01:10 elapsed, 08:49 remaining |‚ñà‚ñè        | 01:10 elapsed, 08:49 remaining |‚ñà‚ñé        | 01:15 elapsed, 08:44 remaining |‚ñà‚ñé        | 01:15 elapsed, 08:44 remaining |‚ñà‚ñé        | 01:20 elapsed, 08:39 remaining |‚ñà‚ñé        | 01:20 elapsed, 08:39 remaining |‚ñà‚ñç        | 01:25 elapsed, 08:34 remaining |‚ñà‚ñç        | 01:25 elapsed, 08:34 remaining |‚ñà‚ñå        | 01:30 elapsed, 08:29 remaining |‚ñà‚ñå        | 01:30 elapsed, 08:29 remaining |‚ñà‚ñå        | 01:35 elapsed, 08:24 remaining |‚ñà‚ñå        | 01:35 elapsed, 08:24 remaining |‚ñà‚ñã        | 01:40 elapsed, 08:19 remaining |‚ñà‚ñã        | 01:40 elapsed, 08:19 remainingINFO 12-19 17:48:53 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:48:53 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:48:53 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:48:53 [__init__.py:241] Automatically detected platform cuda.
 |‚ñà‚ñä        | 01:45 elapsed, 08:14 remaining |‚ñà‚ñä        | 01:45 elapsed, 08:14 remainingWARNING 12-19 17:49:02 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:49:02 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:49:02 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:49:02 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
 |‚ñà‚ñä        | 01:50 elapsed, 08:09 remaining |‚ñà‚ñä        | 01:50 elapsed, 08:09 remaining |‚ñà‚ñâ        | 01:55 elapsed, 08:04 remaining |‚ñà‚ñâ        | 01:55 elapsed, 08:04 remainingINFO 12-19 17:49:09 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2
INFO 12-19 17:49:09 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2
INFO 12-19 17:49:09 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2
INFO 12-19 17:49:09 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c173c423'), local_subscribe_addr='ipc:///tmp/6bb66d8e-9f1f-4f0d-a6d9-397fa648c24f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_75119bf3'), local_subscribe_addr='ipc:///tmp/d511de23-a998-4053-a144-e4dd0f732da7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_113aac1d'), local_subscribe_addr='ipc:///tmp/28bf3c3b-0253-4936-bac7-9e10324582cf', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d98b107b'), local_subscribe_addr='ipc:///tmp/1928f3d7-8141-4bdd-9899-590efe3eb142', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:10 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:10 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:10 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:10 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:10 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:10 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:10 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:10 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:11 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:11 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:11 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:11 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8074ab54'), local_subscribe_addr='ipc:///tmp/83edb594-18a5-4fff-911e-f969c2f1322d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e93c82d9'), local_subscribe_addr='ipc:///tmp/58fa64fe-14f9-45b1-ba8d-d129cc62edf0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:11 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:11 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:11 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:11 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:11 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:11 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:11 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:11 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:11 [parallel_state.py:1134] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:11 [parallel_state.py:1134] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:11 [parallel_state.py:1134] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:11 [parallel_state.py:1134] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m WARNING 12-19 17:49:11 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m WARNING 12-19 17:49:11 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m WARNING 12-19 17:49:11 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m WARNING 12-19 17:49:11 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:11 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:11 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:11 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:11 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:12 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:12 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:12 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:12 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:12 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:12 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:12 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:12 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:12 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:12 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:12 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:12 [weight_utils.py:294] Using model weights format ['*.safetensors']
 |‚ñà‚ñà        | 02:00 elapsed, 07:59 remaining |‚ñà‚ñà        | 02:00 elapsed, 07:59 remaining[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.33it/s]
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:15 [default_loader.py:267] Loading weights took 2.62 seconds
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:15 [default_loader.py:267] Loading weights took 2.63 seconds
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:16 [default_loader.py:267] Loading weights took 2.56 seconds
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:16 [gpu_model_runner.py:1983] Model loading took 3.7718 GiB and 3.441634 seconds
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.62it/s]
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.57it/s]
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m 
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:16 [gpu_model_runner.py:1983] Model loading took 3.7718 GiB and 3.701767 seconds
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:16 [default_loader.py:267] Loading weights took 2.60 seconds
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:16 [gpu_model_runner.py:1983] Model loading took 3.7718 GiB and 3.879747 seconds
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:16 [gpu_model_runner.py:1983] Model loading took 3.7718 GiB and 4.194217 seconds
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:49:17 [gpu_worker.py:276] Available KV cache memory: 28.73 GiB
 |‚ñà‚ñà        | 02:05 elapsed, 07:54 remaining |‚ñà‚ñà        | 02:05 elapsed, 07:54 remaining[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:49:17 [gpu_worker.py:276] Available KV cache memory: 28.73 GiB
 |‚ñà‚ñà‚ñè       | 02:10 elapsed, 07:49 remaining |‚ñà‚ñà‚ñè       | 02:10 elapsed, 07:49 remaining[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:49:25 [gpu_worker.py:276] Available KV cache memory: 31.22 GiB
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:49:25 [gpu_worker.py:276] Available KV cache memory: 31.22 GiB
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:849] GPU KV cache size: 1,022,848 tokens
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 7.80x
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:849] GPU KV cache size: 1,022,848 tokens
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 7.80x
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:849] GPU KV cache size: 941,568 tokens
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 7.18x
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:849] GPU KV cache size: 941,568 tokens
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 7.18x
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:25 [core.py:215] init engine (profile, create kv cache, warmup model) took 9.04 seconds
[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:49:26 [core.py:142] Batch queue is enabled with size 2
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:49:27 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 58848
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:49:27 [async_llm.py:151] Torch profiler enabled. AsyncLLM CPU traces will be collected under /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2
 |‚ñà‚ñà‚ñé       | 02:15 elapsed, 07:44 remaining |‚ñà‚ñà‚ñé       | 02:15 elapsed, 07:44 remaining |‚ñà‚ñà‚ñé       | 02:20 elapsed, 07:39 remaining |‚ñà‚ñà‚ñé       | 02:20 elapsed, 07:39 remaining |‚ñà‚ñà‚ñç       | 02:25 elapsed, 07:34 remaining |‚ñà‚ñà‚ñç       | 02:25 elapsed, 07:34 remaining |‚ñà‚ñà‚ñå       | 02:30 elapsed, 07:29 remaining |‚ñà‚ñà‚ñå       | 02:30 elapsed, 07:29 remaining |‚ñà‚ñà‚ñå       | 02:35 elapsed, 07:24 remaining |‚ñà‚ñà‚ñå       | 02:35 elapsed, 07:24 remaining |‚ñà‚ñà‚ñã       | 02:40 elapsed, 07:19 remaining |‚ñà‚ñà‚ñã       | 02:40 elapsed, 07:19 remaining |‚ñà‚ñà‚ñä       | 02:45 elapsed, 07:14 remaining |‚ñà‚ñà‚ñä       | 02:45 elapsed, 07:14 remaining |‚ñà‚ñà‚ñä       | 02:50 elapsed, 07:09 remaining |‚ñà‚ñà‚ñä       | 02:50 elapsed, 07:09 remaining[1;36m(EngineCore_0 pid=1776861)[0;0m INFO 12-19 17:50:06 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [api_server.py:1679] Supported_tasks: ['generate']
[1;36m(APIServer pid=1776197)[0;0m WARNING 12-19 17:50:06 [__init__.py:1670] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [serving_responses.py:124] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [serving_chat.py:135] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [api_server.py:1950] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:36] Available routes are:
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /start_profile, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /stop_profile, Methods: POST
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:06 [launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1776197)[0;0m INFO:     Started server process [1776197]
[1;36m(APIServer pid=1776197)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=1776197)[0;0m INFO:     Application startup complete.
 |‚ñà‚ñà‚ñâ       | 02:55 elapsed, 07:04 remaining |‚ñà‚ñà‚ñâ       | 02:55 elapsed, 07:04 remaining[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:47462 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m WARNING 12-19 17:50:08 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m WARNING 12-19 17:50:08 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m /global/u1/b/bck/vllm/vllm/distributed/parallel_state.py:522: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1577.)
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[rank1]:[W1219 17:50:08.973872439 ProcessGroupNCCL.cpp:3632] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[rank3]:[W1219 17:50:08.974230713 ProcessGroupNCCL.cpp:3632] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m /global/u1/b/bck/vllm/vllm/distributed/parallel_state.py:522: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1577.)
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[rank0]:[W1219 17:50:08.977923018 ProcessGroupNCCL.cpp:3632] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[rank2]:[W1219 17:50:08.978117595 ProcessGroupNCCL.cpp:3632] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m WARNING 12-19 17:50:08 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m WARNING 12-19 17:50:08 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:17 [loggers.py:123] Engine 000: Avg prompt throughput: 40.9 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
 |‚ñà‚ñà‚ñâ       | 03:05 elapsed, 07:28 remaining
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:17 [api_server.py:1258] Starting profiler...
ERROR: External init callback must run in same thread as registerClient (-1092655360 != -7947392)
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:18 [api_server.py:1260] Profiler started.
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:47462 - "POST /start_profile HTTP/1.1" 200 OK
Initial test run completed. Starting main benchmark run...
Starting profiler...
Traffic request rate: 10.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
  0%|          | 0/50 [00:00<?, ?it/s][1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:47462 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53810 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53812 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53828 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53836 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53848 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53864 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53880 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53892 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53896 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53902 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53906 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53910 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53924 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53926 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53942 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53950 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53956 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53966 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53982 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53988 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:53996 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54006 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54010 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54014 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54024 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54032 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54048 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54060 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54074 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54084 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54098 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54110 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54126 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54130 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54142 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54148 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54152 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54160 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54168 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54170 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54172 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54184 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54188 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54190 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54194 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54198 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54204 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54212 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:54224 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:27 [loggers.py:123] Engine 000: Avg prompt throughput: 10238.4 tokens/s, Avg generation throughput: 874.4 tokens/s, Running: 50 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.8%, Prefix cache hit rate: 1.9%
  2%|‚ñè         | 1/50 [00:18<14:44, 18.05s/it]  4%|‚ñç         | 2/50 [00:18<05:59,  7.50s/it]  8%|‚ñä         | 4/50 [00:18<02:10,  2.85s/it] 12%|‚ñà‚ñè        | 6/50 [00:18<01:07,  1.53s/it] 16%|‚ñà‚ñå        | 8/50 [00:18<00:40,  1.03it/s][1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:37 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1447.3 tokens/s, Running: 41 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.9%, Prefix cache hit rate: 1.9%
 20%|‚ñà‚ñà        | 10/50 [00:19<00:27,  1.45it/s] 24%|‚ñà‚ñà‚ñç       | 12/50 [00:19<00:18,  2.10it/s] 28%|‚ñà‚ñà‚ñä       | 14/50 [00:19<00:12,  2.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:19<00:08,  3.82it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:19<00:06,  5.00it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:19<00:05,  5.74it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:20<00:05,  4.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:20<00:04,  5.89it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:20<00:03,  7.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:21<00:02,  7.56it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:21<00:02,  9.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:21<00:01, 10.39it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:21<00:01, 11.55it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:21<00:01, 12.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:21<00:00, 13.15it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:21<00:00, 10.45it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:22<00:00, 11.72it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:22<00:00, 12.83it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:22<00:00, 13.47it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:22<00:00, 14.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:22<00:00, 15.42it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:22<00:00,  2.21it/s]
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:40 [api_server.py:1265] Stopping profiler...
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:47 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 241.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:50:57 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:55:03 [api_server.py:1267] Profiler stopped.
[1;36m(APIServer pid=1776197)[0;0m INFO:     127.0.0.1:47462 - "POST /stop_profile HTTP/1.1" 200 OK
============ Serving Benchmark Result ============
Successful requests:                     50        
Request rate configured (RPS):           10.00     
Benchmark duration (s):                  22.59     
Total input tokens:                      102350    
Total generated tokens:                  25600     
Request throughput (req/s):              2.21      
Output token throughput (tok/s):         1133.24   
Total Token throughput (tok/s):          5664.00   
---------------Time to First Token----------------
Mean TTFT (ms):                          199.83    
Median TTFT (ms):                        176.98    
P99 TTFT (ms):                           410.29    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          34.35     
Median TPOT (ms):                        34.45     
P99 TPOT (ms):                           34.90     
---------------Inter-token Latency----------------
Mean ITL (ms):                           34.35     
Median ITL (ms):                         33.92     
P99 ITL (ms):                            41.14     
==================================================
Stopping profiler...
Stopping server for llama3_4gpu_tp_2_pp2
[1;36m(APIServer pid=1776197)[0;0m INFO 12-19 17:55:03 [launcher.py:101] Shutting down FastAPI HTTP server.
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                                aten::mm        13.97%        1.640s        19.97%        2.346s      28.419us        5.383s        52.94%        5.386s      65.234us         82560  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.580s        25.37%        2.580s      78.058us         33056  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.81%     447.970ms        19.14%        2.248s     108.923us        2.108s        20.73%        2.295s     111.194us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                _C_custom_ar::all_reduce         1.57%     183.930ms         7.56%     887.971ms      20.859us        1.785s        17.55%        1.787s      41.984us         42570  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        1.663s        16.36%        1.663s      39.073us         42570  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.470s        14.45%        1.470s      85.136us         17264  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.441s        14.17%        1.441s     222.365us          6480  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     853.708ms         8.40%     853.708ms      24.577us         34736  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     504.714ms         4.96%     504.714ms     149.501us          3376  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                             aten::copy_         3.22%     378.165ms         6.13%     720.092ms      12.122us     225.697ms         2.22%     226.342ms       3.810us         59405  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                  _C::fused_add_rms_norm         1.22%     143.850ms         3.28%     385.391ms       9.637us     192.005ms         1.89%     192.464ms       4.813us         39990  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     192.005ms         1.89%     192.005ms       4.801us         39990  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     188.081ms         1.85%     188.081ms      55.711us          3376  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     187.482ms         1.84%     187.482ms      58.297us          3216  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                        _C::silu_and_mul         0.56%      65.383ms         1.72%     202.237ms       9.798us     174.985ms         1.72%     175.072ms       8.482us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     174.985ms         1.72%     174.985ms       8.478us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     139.847ms         1.38%     139.847ms       4.065us         34400  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     121.808ms         1.20%     121.808ms       2.857us         42634  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us     110.369ms         1.09%     110.369ms       7.647us         14432  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                    _C::rotary_embedding         0.79%      93.201ms         2.00%     234.931ms      11.382us      95.452ms         0.94%      95.509ms       4.627us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      95.452ms         0.94%      95.452ms       4.625us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.85%      99.467ms         2.05%     240.469ms      11.651us      87.808ms         0.86%      87.834ms       4.256us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      87.808ms         0.86%      87.808ms       4.254us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      80.230ms         0.79%      80.230ms       4.002us         20048  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                      record_param_comms         1.11%     130.062ms         1.58%     185.780ms      36.004us      45.394ms         0.45%      46.662ms       9.043us          5160  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ncclDevKernel_SendRecv(ncclDevKernelArgsStorage<4096...         0.00%       0.000us         0.00%       0.000us       0.000us      45.394ms         0.45%      45.394ms      17.595us          2580  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                          nccl:send 0->1         0.00%       0.000us         0.00%       0.000us       0.000us      45.394ms         0.45%      45.394ms      17.595us          2580  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.351ms         0.43%      43.351ms       2.520us         17200  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      42.396ms         0.42%      42.396ms       5.478us          7740  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                             aten::fill_         1.21%     142.513ms         2.31%     271.784ms      12.393us      41.294ms         0.41%      41.424ms       1.889us         21930  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      39.624ms         0.39%      39.624ms       1.920us         20640  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      22.292ms         0.22%      22.292ms      23.614us           944  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      13.897ms         0.14%      13.897ms      22.857us           608  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                      aten::index_select         0.16%      18.524ms         0.36%      41.747ms      16.181us      11.166ms         0.11%      11.173ms       4.331us          2580  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       9.937ms         0.10%       9.937ms       7.862us          1264  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.02%       1.895ms         0.02%       1.903ms       0.369us       9.785ms         0.10%       9.785ms       1.895us          5164  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                          cudaEventQuery         0.13%      15.682ms         0.13%      15.784ms       1.894us       9.244ms         0.09%       9.244ms       1.109us          8334  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       8.937ms         0.09%       8.937ms       8.728us          1024  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                            _C::rms_norm         0.04%       4.633ms         0.14%      16.555ms      12.834us       8.025ms         0.08%       8.030ms       6.225us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void vllm::rms_norm_kernel<c10::BFloat16>(c10::BFloa...         0.00%       0.000us         0.00%       0.000us       0.000us       8.025ms         0.08%       8.025ms       6.221us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       6.402ms         0.06%       6.402ms       9.759us           656  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                      aten::masked_fill_         0.12%      14.087ms         0.21%      24.944ms      19.337us       6.384ms         0.06%       6.451ms       5.001us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.384ms         0.06%       6.384ms       4.949us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                         cudaEventRecord         0.13%      15.477ms         0.13%      15.479ms       1.000us       5.590ms         0.05%       5.590ms       0.361us         15480  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       4.725ms         0.05%       4.725ms      98.430us            48  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.693ms         0.05%       4.693ms       2.904us          1616  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.607ms         0.05%       4.607ms       1.384us          3328  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.427ms         0.03%       3.427ms       7.139us           480  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.411ms         0.03%       3.411ms       7.895us           432  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                            Unrecognized         0.47%      54.943ms         0.47%      54.943ms     947.293us       2.700ms         0.03%       2.700ms      46.546us            58  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.12%      14.671ms         0.23%      27.316ms      21.241us       2.498ms         0.02%       2.498ms       1.943us          1286  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.00%       0.000us         0.00%       0.000us       0.000us       2.498ms         0.02%       2.498ms       1.943us          1286  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       2.229ms         0.02%       2.229ms       8.379us           266  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                               aten::sub         0.18%      21.126ms         0.36%      42.782ms      33.139us       2.059ms         0.02%       2.059ms       1.595us          1291  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.059ms         0.02%       2.059ms       1.596us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                            Buffer Flush         0.04%       4.927ms         0.04%       5.077ms      81.891us       1.556ms         0.02%       1.556ms      25.101us            62  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.507ms         0.01%       1.507ms       1.532us           984  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.469ms         0.01%       1.469ms      22.959us            64  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     588.959us         0.01%     588.959us      18.405us            32  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                   Lazy Function Loading         0.01%     603.490us         0.01%     603.490us      67.054us     323.168us         0.00%     323.168us      35.908us             9  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     162.783us         0.00%     162.783us       1.507us           108  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%      53.765us         0.00%     101.056us      25.264us       8.032us         0.00%       8.032us       2.008us             4  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%       0.000us         0.00%       0.000us       0.000us       8.032us         0.00%       8.032us       2.008us             4  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%      33.047us         0.00%      61.080us      15.270us       6.558us         0.00%       6.558us       1.640us             4  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%       0.000us         0.00%       0.000us       0.000us       6.558us         0.00%       6.558us       1.640us             4  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                             aten::slice         0.83%      97.031ms         1.13%     133.103ms       1.877us       0.000us         0.00%       1.152us       0.000us         70929  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                        aten::as_strided         2.12%     249.262ms         2.12%     249.302ms       0.664us       0.000us         0.00%      21.825us       0.000us        375566  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                         cudaMemcpyAsync         4.44%     522.019ms         4.45%     522.208ms      10.367us       0.000us         0.00%     685.342us       0.014us         50374  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                           aten::flatten         0.03%       2.987ms         0.06%       6.676ms       5.175us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                              aten::view         0.80%      93.611ms         0.80%      93.618ms       0.589us       0.000us         0.00%       2.656us       0.000us        158926  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                        aten::lift_fresh         0.00%     503.026us         0.00%     503.026us       0.195us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                        cudaLaunchKernel        17.98%        2.112s        18.19%        2.137s       5.877us       0.000us         0.00%       2.899ms       0.008us        363548  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                TorchDynamo Cache Lookup         0.23%      26.910ms         0.23%      26.920ms      20.868us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                   _compile.compile_inner (dynamo_timed)         0.37%      43.044ms         0.72%      84.686ms      84.686ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                   cudaStreamIsCapturing         0.23%      27.156ms         0.23%      27.156ms       0.601us       0.000us         0.00%       0.000us       0.000us         45156  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                     aten::empty_strided         4.01%     470.784ms         4.05%     475.653ms       5.817us       0.000us         0.00%     294.974us       0.004us         81774  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                            aten::detach         0.00%      38.146us         0.00%      50.779us       2.821us       0.000us         0.00%       0.000us       0.000us            18  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                                  detach         0.00%      12.633us         0.00%      12.633us       6.317us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                                aten::ge         0.01%       1.207ms         0.03%       3.684ms       1.842ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                               prims::ge         0.01%     698.272us         0.02%       2.477ms       1.238ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                    aten::empty_permuted         0.01%       1.723ms         0.04%       4.146ms     460.692us       0.000us         0.00%       0.000us       0.000us             9  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                             aten::empty         5.52%     648.654ms         5.56%     652.934ms       4.238us       0.000us         0.00%     501.793us       0.003us        154075  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                                aten::lt         0.01%     666.633us         0.02%       1.864ms     932.091us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                               prims::lt         0.00%     524.596us         0.01%       1.198ms     598.775us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                           aten::__and__         0.00%     357.884us         0.04%       4.639ms       2.320ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                       aten::bitwise_and         0.02%       2.879ms         0.04%       4.281ms       2.141ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                      prims::bitwise_and         0.01%     698.585us         0.01%       1.402ms     701.009us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                               aten::mul         0.02%       1.763ms         0.02%       2.176ms     725.442us       0.000us         0.00%       0.000us       0.000us             3  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                               aten::add         0.00%     490.421us         0.01%     608.970us     608.970us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                            aten::__or__         0.00%     193.776us         0.01%       1.270ms       1.270ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                        aten::bitwise_or         0.00%     418.350us         0.01%       1.076ms       1.076ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                       prims::bitwise_or         0.00%     332.584us         0.01%     657.454us     657.454us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                       aten::bitwise_not         0.00%     252.930us         0.01%     893.150us     893.150us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                      prims::bitwise_not         0.00%     284.671us         0.01%     640.220us     640.220us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m           OutputGraph.call_user_compiler (dynamo_timed)         0.11%      12.583ms         0.20%      23.949ms      23.949ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m               _recursive_pre_grad_passes (dynamo_timed)         0.01%     779.891us         0.01%     779.891us     779.891us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m              TritonBundler.read_and_emit (dynamo_timed)         0.00%      83.512us         0.00%      83.512us      83.512us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m             PyCodeCache.load_by_key_path (dynamo_timed)         0.01%       1.198ms         0.08%       9.918ms       9.918ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                 async_compile.precompile (dynamo_timed)         0.07%       8.434ms         0.07%       8.720ms       8.720ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m                                   cudaDeviceSynchronize         0.00%     394.736us         0.00%     394.736us     197.368us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m Self CPU time total: 11.747s
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m Self CUDA time total: 10.169s
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m 
[1;36m(VllmWorker PP0_TP0 pid=1777085)[0;0m INFO 12-19 17:55:03 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                                aten::mm        13.94%        1.637s        19.85%        2.331s      28.229us        5.369s        46.49%        5.372s      65.070us         82560  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                _C_custom_ar::all_reduce         1.57%     183.951ms         7.59%     891.376ms      20.939us        3.181s        27.55%        3.182s      74.748us         42570  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        3.060s        26.50%        3.060s      71.871us         42570  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.580s        22.34%        2.580s      78.043us         33056  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.83%     449.580ms        19.18%        2.252s     109.097us        2.107s        18.24%        2.294s     111.127us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.470s        12.73%        1.470s      85.138us         17264  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.431s        12.40%        1.431s     220.900us          6480  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     851.991ms         7.38%     851.991ms      24.528us         34736  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     503.488ms         4.36%     503.488ms     149.137us          3376  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                             aten::copy_         3.16%     371.503ms         6.16%     723.644ms      12.182us     225.492ms         1.95%     226.536ms       3.813us         59405  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                  _C::fused_add_rms_norm         1.21%     142.448ms         3.22%     378.349ms       9.461us     192.959ms         1.67%     193.417ms       4.837us         39990  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     192.959ms         1.67%     192.959ms       4.825us         39990  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     186.650ms         1.62%     186.650ms      55.287us          3376  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     186.507ms         1.62%     186.507ms      57.994us          3216  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                        _C::silu_and_mul         0.55%      64.621ms         1.70%     199.947ms       9.687us     175.023ms         1.52%     175.248ms       8.491us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     175.023ms         1.52%     175.023ms       8.480us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     140.302ms         1.22%     140.302ms       4.079us         34400  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     121.364ms         1.05%     121.364ms       2.847us         42634  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us     110.024ms         0.95%     110.024ms       7.624us         14432  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                    _C::rotary_embedding         0.81%      95.455ms         2.00%     234.778ms      11.375us      94.964ms         0.82%      94.984ms       4.602us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      94.964ms         0.82%      94.964ms       4.601us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.87%     101.661ms         2.09%     245.630ms      11.901us      87.683ms         0.76%      87.749ms       4.251us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      87.683ms         0.76%      87.683ms       4.248us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      80.080ms         0.69%      80.080ms       3.994us         20048  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                      record_param_comms         1.15%     135.280ms         1.67%     195.900ms      37.965us      44.721ms         0.39%      44.754ms       8.673us          5160  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ncclDevKernel_SendRecv(ncclDevKernelArgsStorage<4096...         0.00%       0.000us         0.00%       0.000us       0.000us      44.721ms         0.39%      44.721ms      17.334us          2580  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                          nccl:send 0->1         0.00%       0.000us         0.00%       0.000us       0.000us      44.721ms         0.39%      44.721ms      17.334us          2580  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.146ms         0.37%      43.146ms       2.509us         17200  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      41.921ms         0.36%      41.921ms       5.416us          7740  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                             aten::fill_         1.17%     137.131ms         2.26%     265.766ms      12.119us      41.586ms         0.36%      41.695ms       1.901us         21930  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      39.925ms         0.35%      39.925ms       1.934us         20640  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      22.240ms         0.19%      22.240ms      23.559us           944  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.01%       1.587ms         0.01%       1.587ms       0.307us      14.351ms         0.12%      14.351ms       2.779us          5164  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      13.904ms         0.12%      13.904ms      22.868us           608  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                          cudaEventQuery         0.13%      15.666ms         0.13%      15.751ms       1.891us      13.738ms         0.12%      13.738ms       1.649us          8331  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                      aten::index_select         0.16%      18.684ms         0.35%      41.223ms      15.978us      10.428ms         0.09%      10.432ms       4.043us          2580  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       9.929ms         0.09%       9.929ms       7.855us          1264  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       8.680ms         0.08%       8.680ms       8.477us          1024  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                            _C::rms_norm         0.04%       4.568ms         0.14%      15.892ms      12.320us       7.952ms         0.07%       7.952ms       6.164us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void vllm::rms_norm_kernel<c10::BFloat16>(c10::BFloa...         0.00%       0.000us         0.00%       0.000us       0.000us       7.952ms         0.07%       7.952ms       6.164us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                         cudaEventRecord         0.14%      16.320ms         0.14%      16.322ms       1.054us       6.806ms         0.06%       6.806ms       0.440us         15480  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       6.399ms         0.06%       6.399ms       9.755us           656  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                      aten::masked_fill_         0.12%      14.099ms         0.21%      24.727ms      19.168us       6.141ms         0.05%       6.345ms       4.919us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.141ms         0.05%       6.141ms       4.760us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       4.731ms         0.04%       4.731ms      98.554us            48  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.727ms         0.04%       4.727ms       2.925us          1616  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       4.547ms         0.04%       4.547ms       1.366us          3328  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.444ms         0.03%       3.444ms       7.174us           480  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.404ms         0.03%       3.404ms       7.879us           432  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.12%      14.671ms         0.23%      27.509ms      21.391us       2.486ms         0.02%       2.486ms       1.933us          1286  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m triton_poi_fused_add_bitwise_and_bitwise_not_bitwise...         0.00%       0.000us         0.00%       0.000us       0.000us       2.486ms         0.02%       2.486ms       1.933us          1286  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                               aten::sub         0.18%      20.985ms         0.36%      42.739ms      33.105us       2.048ms         0.02%       2.298ms       1.780us          1291  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.048ms         0.02%       2.048ms       1.587us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       1.748ms         0.02%       1.748ms       6.572us           266  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.500ms         0.01%       1.500ms       1.524us           984  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.473ms         0.01%       1.473ms      23.014us            64  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                            Buffer Flush         0.03%       2.979ms         0.03%       3.183ms      51.339us       1.266ms         0.01%       1.507ms      24.308us            62  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                            Unrecognized         0.46%      54.121ms         0.46%      54.121ms     933.117us       1.235ms         0.01%       1.235ms      21.289us            58  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     586.901us         0.01%     586.901us      18.341us            32  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                   Lazy Function Loading         0.00%     449.904us         0.00%     449.904us      49.989us     323.579us         0.00%     323.579us      35.953us             9  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     161.690us         0.00%     161.690us       1.497us           108  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%      59.895us         0.00%     105.383us      26.346us       7.775us         0.00%       7.775us       1.944us             4  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m triton_poi_fused_add_bitwise_and_bitwise_or_ge_lt_mu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.775us         0.00%       7.775us       1.944us             4  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%      31.212us         0.00%      60.328us      15.082us       6.527us         0.00%       6.527us       1.632us             4  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m triton_poi_fused_bitwise_and_bitwise_not_bitwise_or_...         0.00%       0.000us         0.00%       0.000us       0.000us       6.527us         0.00%       6.527us       1.632us             4  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                             aten::slice         0.83%      97.305ms         1.14%     133.553ms       1.883us       0.000us         0.00%       0.000us       0.000us         70929  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                        aten::as_strided         2.10%     247.167ms         2.11%     247.210ms       0.658us       0.000us         0.00%       3.968us       0.000us        375566  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                         cudaMemcpyAsync         4.49%     527.192ms         4.49%     527.405ms      10.470us       0.000us         0.00%       1.022ms       0.020us         50374  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                           aten::flatten         0.03%       3.138ms         0.06%       6.868ms       5.324us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                              aten::view         0.81%      95.509ms         0.81%      95.511ms       0.601us       0.000us         0.00%       0.000us       0.000us        158926  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                        aten::lift_fresh         0.00%     520.814us         0.00%     520.814us       0.202us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                        cudaLaunchKernel        17.89%        2.100s        17.99%        2.113s       5.812us       0.000us         0.00%       2.019ms       0.006us        363548  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                TorchDynamo Cache Lookup         0.24%      27.748ms         0.24%      27.761ms      21.520us       0.000us         0.00%     284.605us       0.221us          1290  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                   _compile.compile_inner (dynamo_timed)         0.36%      41.738ms         0.69%      81.425ms      81.425ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                   cudaStreamIsCapturing         0.23%      27.575ms         0.23%      27.576ms       0.611us       0.000us         0.00%       0.000us       0.000us         45156  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                     aten::empty_strided         3.99%     468.996ms         4.03%     473.246ms       5.787us       0.000us         0.00%     556.859us       0.007us         81774  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                            aten::detach         0.00%      40.680us         0.00%      52.353us       2.909us       0.000us         0.00%       0.000us       0.000us            18  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                                  detach         0.00%      11.673us         0.00%      11.673us       5.836us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                                aten::ge         0.01%     903.611us         0.03%       2.976ms       1.488ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                               prims::ge         0.01%     611.444us         0.02%       2.072ms       1.036ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                    aten::empty_permuted         0.01%       1.533ms         0.03%       3.701ms     411.231us       0.000us         0.00%       0.000us       0.000us             9  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                             aten::empty         5.43%     637.965ms         5.49%     645.153ms       4.187us       0.000us         0.00%     494.137us       0.003us        154075  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                                aten::lt         0.01%     625.142us         0.02%       1.836ms     918.055us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                               prims::lt         0.00%     543.754us         0.01%       1.211ms     605.484us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                           aten::__and__         0.00%     308.186us         0.04%       4.186ms       2.093ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                       aten::bitwise_and         0.02%       2.501ms         0.03%       3.878ms       1.939ms       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                      prims::bitwise_and         0.01%     704.837us         0.01%       1.377ms     688.750us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                               aten::mul         0.01%       1.691ms         0.02%       2.115ms     704.969us       0.000us         0.00%       0.000us       0.000us             3  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                               aten::add         0.00%     481.022us         0.01%     602.348us     602.348us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                            aten::__or__         0.00%     166.253us         0.01%       1.240ms       1.240ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                        aten::bitwise_or         0.00%     400.245us         0.01%       1.074ms       1.074ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                       prims::bitwise_or         0.00%     343.716us         0.01%     673.335us     673.335us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                       aten::bitwise_not         0.00%     203.875us         0.01%     797.706us     797.706us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                      prims::bitwise_not         0.00%     269.042us         0.01%     593.831us     593.831us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m           OutputGraph.call_user_compiler (dynamo_timed)         0.10%      12.129ms         0.20%      24.064ms      24.064ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m               _recursive_pre_grad_passes (dynamo_timed)         0.01%     768.299us         0.01%     768.299us     768.299us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m              TritonBundler.read_and_emit (dynamo_timed)         0.00%      86.478us         0.00%      86.478us      86.478us       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m             PyCodeCache.load_by_key_path (dynamo_timed)         0.01%       1.014ms         0.09%      10.539ms      10.539ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                 async_compile.precompile (dynamo_timed)         0.08%       9.357ms         0.08%       9.526ms       9.526ms       0.000us         0.00%       0.000us       0.000us             1  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m                                   cudaDeviceSynchronize         0.00%     283.269us         0.00%     283.269us     141.634us       0.000us         0.00%       0.000us       0.000us             2  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m Self CPU time total: 11.742s
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m Self CUDA time total: 11.547s
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m 
[1;36m(VllmWorker PP0_TP1 pid=1777086)[0;0m INFO 12-19 17:55:03 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                                aten::mm        12.44%        1.692s        17.61%        2.396s      28.573us        5.890s        55.56%        5.898s      70.337us         83850  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.571s        24.25%        2.571s      77.774us         33056  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.39%     461.813ms        17.04%        2.319s     112.334us        2.110s        19.90%        2.299s     111.399us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.470s        13.87%        1.470s      85.176us         17264  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.448s        13.66%        1.448s     223.438us          6480  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                _C_custom_ar::all_reduce         1.37%     186.844ms         6.43%     875.528ms      21.209us        1.228s        11.59%        1.234s      29.883us         41280  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        1.110s        10.47%        1.110s      26.888us         41280  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     854.945ms         8.06%     854.945ms      24.613us         34736  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     505.142ms         4.76%     505.142ms     149.627us          3376  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                      record_param_comms         2.24%     305.222ms         3.23%     439.429ms      34.064us     430.825ms         4.06%     433.799ms      33.628us         12900  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     346.976ms         3.27%     346.976ms     395.189us           878  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                             aten::copy_         3.33%     452.972ms         6.92%     941.767ms      13.263us     274.447ms         2.59%     281.760ms       3.968us         71005  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsSto...         0.00%       0.000us         0.00%       0.000us       0.000us     224.018ms         2.11%     224.018ms      57.886us          3870  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us         0.00%       0.000us       0.000us     224.018ms         2.11%     224.018ms      57.886us          3870  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ncclDevKernel_SendRecv(ncclDevKernelArgsStorage<4096...         0.00%       0.000us         0.00%       0.000us       0.000us     206.807ms         1.95%     206.807ms      80.158us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                          nccl:recv 1<-0         0.00%       0.000us         0.00%       0.000us       0.000us     206.807ms         1.95%     206.807ms      80.158us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                  _C::fused_add_rms_norm         1.15%     156.136ms         3.02%     411.464ms       9.666us     204.750ms         1.93%     205.782ms       4.834us         42570  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     204.750ms         1.93%     204.750ms       4.810us         42570  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     188.653ms         1.78%     188.653ms      55.881us          3376  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     188.091ms         1.77%     188.091ms      58.486us          3216  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                        _C::silu_and_mul         0.48%      65.124ms         1.46%     197.992ms       9.593us     176.403ms         1.66%     176.609ms       8.557us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     176.403ms         1.66%     176.403ms       8.547us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     155.654ms         1.47%     155.654ms       4.362us         35681  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us     151.417ms         1.43%     151.417ms     388.248us           390  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     125.498ms         1.18%     125.498ms       2.857us         43924  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us     110.871ms         1.05%     110.871ms       7.682us         14432  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                    _C::rotary_embedding         0.71%      97.157ms         1.72%     233.663ms      11.321us      92.654ms         0.87%      93.015ms       4.507us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      92.654ms         0.87%      92.654ms       4.489us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.79%     106.878ms         1.82%     247.673ms      12.000us      87.112ms         0.82%      88.232ms       4.275us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      87.112ms         0.82%      87.112ms       4.221us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      79.643ms         0.75%      79.643ms       3.973us         20048  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            aten::argmax         0.13%      18.062ms         0.20%      27.714ms      21.484us      54.446ms         0.51%      54.566ms      42.299us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      54.446ms         0.51%      54.446ms      42.206us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      42.495ms         0.40%      42.495ms       2.471us         17200  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      41.781ms         0.39%      41.781ms       5.398us          7740  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                             aten::fill_         1.04%     141.863ms         1.99%     271.411ms      12.376us      41.633ms         0.39%      42.184ms       1.924us         21930  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      39.971ms         0.38%      39.971ms       1.937us         20640  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                          cudaEventQuery         0.23%      31.225ms         0.23%      31.544ms       1.995us      28.812ms         0.27%      28.823ms       1.823us         15810  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.03%       4.407ms         0.03%       4.410ms       0.340us      26.114ms         0.25%      26.114ms       2.016us         12954  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      22.601ms         0.21%      22.601ms      23.942us           944  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      16.347ms         0.15%      16.347ms      12.672us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      14.075ms         0.13%      14.075ms      23.150us           608  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       9.984ms         0.09%       9.984ms     163.667us            61  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       9.945ms         0.09%       9.945ms       7.868us          1264  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                             aten::index         0.16%      21.873ms         0.48%      65.534ms      50.801us       8.406ms         0.08%      11.917ms       9.238us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.406ms         0.08%       8.406ms       6.516us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                         cudaEventRecord         0.26%      35.582ms         0.26%      35.587ms       0.985us       7.847ms         0.07%       7.908ms       0.219us         36120  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       6.463ms         0.06%       6.463ms       9.851us           656  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       6.213ms         0.06%       6.213ms       2.408us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       5.123ms         0.05%       5.123ms       1.378us          3718  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            Unrecognized         0.43%      58.590ms         0.43%      58.590ms     944.994us       4.799ms         0.05%       4.799ms      77.404us            62  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.696ms         0.04%       4.696ms       2.906us          1616  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.946ms         0.04%       3.946ms      96.254us            41  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.478ms         0.03%       3.478ms       7.245us           480  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.438ms         0.03%       3.438ms       7.959us           432  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.970ms         0.03%       2.970ms       2.302us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                    cudaEventSynchronize         7.68%        1.045s         7.69%        1.046s     810.961us       2.036ms         0.02%       5.840ms       4.527us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                               aten::sub         0.14%      19.521ms         0.29%      38.986ms      30.222us       1.985ms         0.02%       1.985ms       1.539us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.985ms         0.02%       1.985ms       1.539us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                          Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.869ms         0.02%       1.869ms       1.449us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            Buffer Flush         0.04%       5.696ms         0.04%       5.838ms      88.462us       1.554ms         0.01%       1.554ms      23.549us            66  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.505ms         0.01%       1.505ms      23.511us            64  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.499ms         0.01%       1.499ms       1.524us           984  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                   Lazy Function Loading         0.00%     527.253us         0.00%     527.253us      75.322us     322.593us         0.00%     322.593us      46.085us             7  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     162.307us         0.00%     162.307us       1.503us           108  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                             aten::empty         5.39%     733.200ms         5.47%     744.086ms       4.599us       0.000us         0.00%       1.108ms       0.007us        161799  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                             c10d::recv_         0.67%      90.988ms         1.96%     266.570ms      51.661us       0.000us         0.00%     207.765ms      40.265us          5160  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                               gloo:recv         0.00%       0.000us             0     761.342ms     295.094us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                              aten::item         0.02%       2.170ms         0.02%       2.788ms       2.161us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                               aten::_local_scalar_dense         0.00%     616.330us         0.00%     616.330us       0.478us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            aten::detach         0.01%       1.391ms         0.01%       1.391ms       1.078us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                                aten::to         0.05%       6.243ms         0.84%     114.647ms      14.812us       0.000us         0.00%      25.810ms       3.335us          7740  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                      aten::resolve_conj         0.00%     483.194us         0.00%     483.194us       0.187us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                       aten::resolve_neg         0.00%     253.948us         0.00%     253.948us       0.098us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                           aten::reshape         1.50%     203.665ms         7.93%        1.079s      10.736us       0.000us         0.00%     132.380ms       1.317us        100510  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                              aten::view         0.76%     103.442ms         0.76%     103.458ms       0.607us       0.000us         0.00%     152.543us       0.001us        170545  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            aten::select         0.63%      86.155ms         0.85%     115.885ms       2.642us       0.000us         0.00%     438.560us       0.010us         43860  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                        aten::as_strided         1.89%     256.835ms         1.89%     256.883ms       0.653us       0.000us         0.00%     173.312us       0.000us        393621  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                     cudaStreamWaitEvent         0.16%      21.425ms         0.16%      21.427ms       0.830us       0.000us         0.00%      60.513us       0.002us         25800  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                          nccl:recv 1<-0         0.00%       0.000us             0     122.352ms      47.423us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                             cudaStreamGetCaptureInfo_v2         0.03%       4.544ms         0.03%       4.544ms       0.704us       0.000us         0.00%       0.000us       0.000us          6450  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                     cudaGetFuncBySymbol         0.01%       1.858ms         0.01%       1.858ms       0.288us       0.000us         0.00%       0.000us       0.000us          6450  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                        cuLaunchKernelEx         0.46%      63.138ms         0.47%      64.199ms       9.953us       0.000us         0.00%      98.432us       0.015us          6450  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                   cudaStreamIsCapturing         0.24%      32.128ms         0.24%      32.132ms       0.673us       0.000us         0.00%     305.311us       0.006us         47733  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                        vllm::all_gather         1.77%     241.088ms         4.74%     645.241ms     166.729us       0.000us         0.00%     242.532ms      62.670us          3870  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                  c10d::_allgather_base_         0.27%      37.290ms         2.05%     279.523ms      72.228us       0.000us         0.00%     226.708ms      58.581us          3870  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us             0     176.694ms      45.657us       0.000us         0.00%       0.000us       0.000us          3870  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                           aten::movedim         0.07%       9.620ms         0.14%      19.427ms       5.017us       0.000us         0.00%       0.000us       0.000us          3872  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                           aten::permute         0.05%       6.286ms         0.07%       9.793ms       2.530us       0.000us         0.00%       0.000us       0.000us          3870  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                             aten::slice         0.80%     109.364ms         1.09%     148.044ms       1.822us       0.000us         0.00%      66.336us       0.001us         81249  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                         cudaMemcpyAsync         3.99%     542.457ms         4.00%     544.051ms      10.274us       0.000us         0.00%       1.717ms       0.032us         52954  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                           aten::flatten         0.02%       2.744ms         0.05%       6.459ms       5.007us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                        aten::lift_fresh         0.00%     262.978us         0.00%     262.978us       0.204us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                      aten::index_select         0.02%       2.711ms         0.02%       2.711ms       2.101us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                        cudaLaunchKernel        15.84%        2.155s        15.99%        2.176s       5.861us       0.000us         0.00%       6.753ms       0.018us        371279  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            aten::linear         1.47%     199.436ms        23.05%        3.136s      37.405us       0.000us         0.00%        5.900s      70.362us         83850  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                                 aten::t         1.26%     171.024ms         2.85%     387.674ms       4.623us       0.000us         0.00%     723.391us       0.009us         83850  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                         aten::transpose         1.30%     176.707ms         2.23%     302.792ms       2.232us       0.000us         0.00%     376.032us       0.003us        135642  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m                                            aten::matmul         1.12%     152.611ms        18.73%        2.549s      30.396us       0.000us         0.00%        5.898s      70.341us         83850  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m           cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.30%      40.869ms         0.30%      40.871ms       1.263us       0.000us         0.00%      86.816us       0.003us         32360  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m Self CPU time total: 13.606s
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m Self CUDA time total: 10.601s
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m 
[1;36m(VllmWorker PP1_TP0 pid=1777087)[0;0m INFO 12-19 17:55:03 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                                aten::mm        12.37%        1.720s        17.41%        2.422s      28.881us        5.859s        38.61%        5.872s      70.024us         83850  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                _C_custom_ar::all_reduce         1.26%     175.264ms         6.09%     846.855ms      20.515us        5.501s        36.26%        5.505s     133.359us         41280  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void vllm::cross_device_reduce_1stage<__nv_bfloat16,...         0.00%       0.000us         0.00%       0.000us       0.000us        5.384s        35.48%        5.384s     130.419us         41280  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us        2.569s        16.93%        2.569s      77.724us         33056  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                 _vllm_fa2_C::varlen_fwd         3.26%     453.782ms        16.19%        2.252s     109.108us        2.106s        13.88%        2.309s     111.848us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us        1.472s         9.70%        1.472s      85.238us         17264  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.423s         9.38%        1.423s     219.530us          6480  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     856.770ms         5.65%     856.770ms      24.665us         34736  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                      record_param_comms         2.09%     290.275ms         3.01%     418.877ms      32.471us     757.039ms         4.99%     759.970ms      58.912us         12900  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsSto...         0.00%       0.000us         0.00%       0.000us       0.000us     532.807ms         3.51%     532.807ms     137.676us          3870  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us         0.00%       0.000us       0.000us     532.807ms         3.51%     532.807ms     137.676us          3870  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_kernel<Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     501.177ms         3.30%     501.177ms     148.453us          3376  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     347.124ms         2.29%     347.124ms     395.357us           878  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                             aten::copy_         3.11%     432.968ms         6.54%     909.359ms      12.807us     285.512ms         1.88%     293.262ms       4.130us         71005  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ncclDevKernel_SendRecv(ncclDevKernelArgsStorage<4096...         0.00%       0.000us         0.00%       0.000us       0.000us     224.232ms         1.48%     224.232ms      86.912us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                          nccl:recv 1<-0         0.00%       0.000us         0.00%       0.000us       0.000us     224.232ms         1.48%     224.232ms      86.912us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                  _C::fused_add_rms_norm         1.09%     151.389ms         2.90%     403.140ms       9.470us     203.822ms         1.34%     205.276ms       4.822us         42570  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     203.822ms         1.34%     203.822ms       4.788us         42570  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     185.606ms         1.22%     185.606ms      54.978us          3376  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     185.041ms         1.22%     185.041ms      57.538us          3216  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                        _C::silu_and_mul         0.45%      62.860ms         1.38%     191.668ms       9.286us     175.576ms         1.16%     176.100ms       8.532us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void vllm::act_and_mul_kernel<c10::BFloat16, &(c10::...         0.00%       0.000us         0.00%       0.000us       0.000us     175.576ms         1.16%     175.576ms       8.507us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     155.824ms         1.03%     155.824ms       4.367us         35681  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us     151.331ms         1.00%     151.331ms     388.029us           390  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                          Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     124.919ms         0.82%     124.919ms       2.844us         43924  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us     109.968ms         0.72%     109.968ms       7.620us         14432  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                    _C::rotary_embedding         0.67%      93.301ms         1.67%     232.596ms      11.269us      92.092ms         0.61%      93.200ms       4.516us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void vllm::rotary_embedding_kernel<c10::BFloat16, tr...         0.00%       0.000us         0.00%       0.000us       0.000us      92.092ms         0.61%      92.092ms       4.462us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                   _C_cache_ops::reshape_and_cache_flash         0.73%     101.059ms         1.76%     244.987ms      11.870us      86.074ms         0.57%      86.876ms       4.209us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void vllm::reshape_and_cache_flash_kernel<__nv_bfloa...         0.00%       0.000us         0.00%       0.000us       0.000us      86.074ms         0.57%      86.074ms       4.170us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us      79.600ms         0.52%      79.600ms       3.970us         20048  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            aten::argmax         0.12%      17.199ms         0.18%      24.951ms      19.342us      54.226ms         0.36%      54.226ms      42.035us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      54.226ms         0.36%      54.226ms      42.035us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      52.372ms         0.35%      52.372ms       3.045us         17200  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                             aten::fill_         0.99%     138.292ms         1.90%     263.618ms      12.021us      41.986ms         0.28%      42.838ms       1.953us         21930  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                          Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      41.579ms         0.27%      41.579ms       5.372us          7740  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                          cudaEventQuery         0.23%      32.457ms         0.24%      32.764ms       2.068us      40.281ms         0.27%      40.281ms       2.542us         15846  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      40.077ms         0.26%      40.077ms       1.942us         20640  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                     cudaThreadExchangeStreamCaptureMode         0.03%       4.305ms         0.03%       4.308ms       0.333us      38.632ms         0.25%      38.632ms       2.983us         12952  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      22.648ms         0.15%      22.648ms      23.992us           944  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      16.373ms         0.11%      16.373ms      12.692us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm...         0.00%       0.000us         0.00%       0.000us       0.000us      14.044ms         0.09%      14.044ms      23.099us           608  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                         cudaEventRecord         0.25%      34.326ms         0.25%      34.329ms       0.950us      10.397ms         0.07%      10.397ms       0.288us         36120  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_256x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us       9.980ms         0.07%       9.980ms     163.613us            61  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       9.923ms         0.07%       9.923ms       7.850us          1264  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                             aten::index         0.15%      21.368ms         0.47%      65.710ms      50.938us       8.394ms         0.06%      12.029ms       9.325us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.394ms         0.06%       8.394ms       6.507us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.055ms         0.05%       7.055ms       2.735us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       6.482ms         0.04%       6.482ms       9.881us           656  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                    cudaEventSynchronize        11.29%        1.571s        11.31%        1.573s       1.219ms       5.154ms         0.03%      15.204ms      11.786us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       5.039ms         0.03%       5.039ms       1.355us          3718  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       4.670ms         0.03%       4.670ms       2.890us          1616  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m ampere_bf16_s16816gemm_bf16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.922ms         0.03%       3.922ms      95.647us            41  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.427ms         0.02%       3.427ms       7.140us           480  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void flash::flash_fwd_splitkv_combine_kernel<Flash_f...         0.00%       0.000us         0.00%       0.000us       0.000us       3.404ms         0.02%       3.404ms       7.880us           432  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.244ms         0.02%       3.244ms       2.515us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            Unrecognized         0.44%      60.517ms         0.44%      60.517ms     976.084us       3.152ms         0.02%       3.152ms      50.832us            62  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                               aten::sub         0.14%      19.043ms         0.27%      38.192ms      29.606us       2.575ms         0.02%       2.587ms       2.005us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.575ms         0.02%       2.575ms       1.996us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                          Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       1.855ms         0.01%       1.855ms       1.438us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.723ms         0.01%       1.723ms       1.751us           984  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m       ampere_s16816gemm_bf16_128x64_ldg8_stages_64x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.501ms         0.01%       1.501ms      23.456us            64  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            Buffer Flush         0.03%       3.716ms         0.03%       3.884ms      58.847us     803.225us         0.01%     803.225us      12.170us            66  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                   Lazy Function Loading         0.00%     532.542us         0.00%     532.542us      76.077us     318.431us         0.00%     318.431us      45.490us             7  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     186.212us         0.00%     186.212us       1.724us           108  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                             aten::empty         5.07%     704.915ms         5.14%     715.181ms       4.420us       0.000us         0.00%       2.206ms       0.014us        161799  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                             c10d::recv_         0.66%      91.716ms         1.84%     255.824ms      49.578us       0.000us         0.00%     225.299ms      43.663us          5160  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                               gloo:recv         0.00%       0.000us             0     716.654ms     277.773us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                              aten::item         0.01%       2.079ms         0.02%       2.681ms       2.078us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                               aten::_local_scalar_dense         0.00%     598.248us         0.00%     598.248us       0.464us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            aten::detach         0.01%       1.410ms         0.01%       1.411ms       1.093us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                                aten::to         0.04%       6.062ms         0.83%     114.926ms      14.848us       0.000us         0.00%      26.677ms       3.447us          7740  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                      aten::resolve_conj         0.00%     444.540us         0.00%     444.540us       0.172us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                       aten::resolve_neg         0.00%     275.549us         0.00%     275.549us       0.107us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                           aten::reshape         1.44%     200.875ms         7.49%        1.041s      10.361us       0.000us         0.00%     141.445ms       1.407us        100510  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                              aten::view         0.76%     105.218ms         0.76%     105.233ms       0.617us       0.000us         0.00%      52.512us       0.000us        170545  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            aten::select         0.61%      85.341ms         0.83%     115.602ms       2.636us       0.000us         0.00%     262.304us       0.006us         43860  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                        aten::as_strided         1.86%     259.319ms         1.87%     259.373ms       0.659us       0.000us         0.00%     800.702us       0.002us        393621  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                     cudaStreamWaitEvent         0.15%      20.701ms         0.15%      20.704ms       0.802us       0.000us         0.00%       0.000us       0.000us         25800  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                          nccl:recv 1<-0         0.00%       0.000us             0     114.862ms      44.520us       0.000us         0.00%       0.000us       0.000us          2580  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                             cudaStreamGetCaptureInfo_v2         0.03%       4.507ms         0.03%       4.507ms       0.699us       0.000us         0.00%       0.000us       0.000us          6450  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                     cudaGetFuncBySymbol         0.01%       1.613ms         0.01%       1.613ms       0.250us       0.000us         0.00%       0.000us       0.000us          6450  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                        cuLaunchKernelEx         0.44%      60.798ms         0.44%      60.844ms       9.433us       0.000us         0.00%       3.424us       0.001us          6450  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                   cudaStreamIsCapturing         0.23%      31.611ms         0.23%      31.613ms       0.662us       0.000us         0.00%       0.000us       0.000us         47733  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                        vllm::all_gather         1.72%     238.647ms         4.53%     629.662ms     162.703us       0.000us         0.00%     551.032ms     142.386us          3870  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                  c10d::_allgather_base_         0.27%      37.013ms         1.95%     271.461ms      70.145us       0.000us         0.00%     535.284ms     138.316us          3870  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                   nccl:_all_gather_base         0.00%       0.000us             0     170.866ms      44.151us       0.000us         0.00%       0.000us       0.000us          3870  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                           aten::movedim         0.07%       9.748ms         0.14%      19.678ms       5.083us       0.000us         0.00%       0.000us       0.000us          3871  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                           aten::permute         0.05%       6.387ms         0.07%       9.923ms       2.564us       0.000us         0.00%       0.000us       0.000us          3870  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                             aten::slice         0.79%     109.371ms         1.08%     149.930ms       1.845us       0.000us         0.00%     108.224us       0.001us         81249  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                         cudaMemcpyAsync         3.82%     531.172ms         3.82%     531.699ms      10.041us       0.000us         0.00%       1.283ms       0.024us         52954  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                           aten::flatten         0.02%       2.799ms         0.05%       6.393ms       4.956us       0.000us         0.00%       2.880us       0.002us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                        aten::lift_fresh         0.00%     256.140us         0.00%     256.140us       0.199us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                      aten::index_select         0.02%       2.573ms         0.02%       2.576ms       1.997us       0.000us         0.00%       0.000us       0.000us          1290  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                        cudaLaunchKernel        15.29%        2.127s        15.42%        2.144s       5.774us       0.000us         0.00%       7.005ms       0.019us        371279  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            aten::linear         1.37%     190.785ms        22.71%        3.158s      37.662us       0.000us         0.00%        5.874s      70.054us         83850  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                                 aten::t         1.23%     170.398ms         2.85%     396.226ms       4.725us       0.000us         0.00%       1.393ms       0.017us         83850  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                         aten::transpose         1.33%     185.378ms         2.24%     311.523ms       2.297us       0.000us         0.00%     647.934us       0.005us        135642  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m                                            aten::matmul         1.07%     148.719ms        18.48%        2.571s      30.656us       0.000us         0.00%        5.872s      70.032us         83850  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m           cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.29%      40.342ms         0.29%      40.345ms       1.247us       0.000us         0.00%       6.176us       0.000us         32360  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m Self CPU time total: 13.906s
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m Self CUDA time total: 15.174s
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m 
[1;36m(VllmWorker PP1_TP1 pid=1777088)[0;0m INFO 12-19 17:55:03 [multiproc_executor.py:526] Parent process exited, terminating worker
[rank2]:[W1219 17:55:04.458394698 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=93, addr=[localhost]:35982, remote=[localhost]:36739): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7efca27785e8 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7efc8bed5bfe in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baaf40 (0x7efc8bed7f40 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab84a (0x7efc8bed884a in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7efc8bed22a9 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7efc4d5d19f9 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xe8964 (0x7efc3d3bc964 in /usr/lib64/libstdc++.so.6)
frame #7: <unknown function> + 0xa6ea (0x7efca3b7f6ea in /lib64/libpthread.so.0)
frame #8: clone + 0x41 (0x7efca393553f in /lib64/libc.so.6)

[rank2]:[W1219 17:55:04.471030984 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 2] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[rank3]:[W1219 17:55:04.483968824 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=93, addr=[localhost]:35998, remote=[localhost]:36739): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7ff8c07785e8 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7ff8a9ed5bfe in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baaf40 (0x7ff8a9ed7f40 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab84a (0x7ff8a9ed884a in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7ff8a9ed22a9 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7ff86b5d19f9 in /global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xe8964 (0x7ff85b3bc964 in /usr/lib64/libstdc++.so.6)
frame #7: <unknown function> + 0xa6ea (0x7ff8c1b776ea in /lib64/libpthread.so.0)
frame #8: clone + 0x41 (0x7ff8c192d53f in /lib64/libc.so.6)

[rank3]:[W1219 17:55:04.487301412 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 3] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[1;36m(APIServer pid=1776197)[0;0m INFO:     Shutting down
[1;36m(APIServer pid=1776197)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=1776197)[0;0m INFO:     Application shutdown complete.
Traces for llama3_4gpu_tp_2_pp2 are in: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_tp_2_pp2

=== Running experiment: llama3_4gpu_dp2_pp2 ===
Model=meta-llama/Llama-3.1-8B DP=2 TP=1 PP=2 GPUs=0,1,2,3
Trace dir: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_dp2_pp2
Server PID: 1779722
INFO 12-19 17:55:44 [__init__.py:241] Automatically detected platform cuda.
WARNING 12-19 17:55:53 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:55:53 [__init__.py:1750] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:55:53 [api_server.py:1875] vLLM API server version 0.10.1rc2.dev263+g2f13319f4
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:55:53 [utils.py:326] non-default args: {'model': 'meta-llama/Llama-3.1-8B', 'trust_remote_code': True, 'enforce_eager': True, 'disable_sliding_window': True, 'pipeline_parallel_size': 2, 'data_parallel_size': 2, 'swap_space': 16.0, 'max_num_batched_tokens': 512, 'max_num_seqs': 512, 'enable_chunked_prefill': True}
[1;36m(APIServer pid=1779722)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-19 17:56:02 [__init__.py:241] Automatically detected platform cuda.
WARNING 12-19 17:56:14 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fefadb80b80>, seed=0, num_prompts=50, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=2048, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, endpoint_type='openai', label=None, backend='vllm', base_url='http://127.0.0.1:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=10.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=True, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600)
INFO 12-19 17:56:15 [datasets.py:509] Sampling input_len from [2047, 2047] and output_len from [512, 512]
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
 |          | 00:00 elapsed, ? remaining |          | 00:00 elapsed, 00:01 remaining |          | 00:05 elapsed, 09:54 remaining |          | 00:05 elapsed, 09:54 remaining |‚ñè         | 00:10 elapsed, 09:49 remaining |‚ñè         | 00:10 elapsed, 09:49 remaining |‚ñé         | 00:15 elapsed, 09:44 remaining |‚ñé         | 00:15 elapsed, 09:44 remaining[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:56:33 [__init__.py:742] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1779722)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:56:33 [__init__.py:1786] Using max model len 131072
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:56:35 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=512.
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:56:35 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:56:36 [utils.py:625] Started DP Coordinator process (PID: 1780402)
 |‚ñé         | 00:20 elapsed, 09:39 remaining |‚ñé         | 00:20 elapsed, 09:39 remaining |‚ñç         | 00:25 elapsed, 09:34 remaining |‚ñç         | 00:25 elapsed, 09:34 remaining |‚ñå         | 00:30 elapsed, 09:29 remaining |‚ñå         | 00:30 elapsed, 09:29 remaining |‚ñå         | 00:35 elapsed, 09:24 remaining |‚ñå         | 00:35 elapsed, 09:24 remaining |‚ñã         | 00:40 elapsed, 09:19 remaining |‚ñã         | 00:40 elapsed, 09:19 remaining |‚ñä         | 00:45 elapsed, 09:14 remaining |‚ñä         | 00:45 elapsed, 09:14 remaining |‚ñä         | 00:50 elapsed, 09:09 remaining |‚ñä         | 00:50 elapsed, 09:09 remainingINFO 12-19 17:57:10 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:57:10 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:57:10 [__init__.py:241] Automatically detected platform cuda.
 |‚ñâ         | 00:55 elapsed, 09:04 remaining |‚ñâ         | 00:55 elapsed, 09:04 remaining |‚ñà         | 01:00 elapsed, 08:59 remaining |‚ñà         | 01:00 elapsed, 08:59 remainingWARNING 12-19 17:57:20 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:57:20 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:57:20 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:57:20 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:57:20 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:57:20 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev263+g2f13319f4) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=1780405)[0;0m WARNING 12-19 17:57:20 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:57:20 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev263+g2f13319f4) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_1 pid=1780406)[0;0m WARNING 12-19 17:57:20 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:57:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_3bd904ec'), local_subscribe_addr='ipc:///tmp/e747da17-5ca8-404b-9833-2f1d3faa4301', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:57:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_81231d2c'), local_subscribe_addr='ipc:///tmp/bbae00d7-e21e-4998-a23b-d30024a23706', remote_subscribe_addr=None, remote_addr_ipv6=False)
 |‚ñà         | 01:05 elapsed, 08:54 remaining |‚ñà         | 01:05 elapsed, 08:54 remaining |‚ñà‚ñè        | 01:10 elapsed, 08:49 remaining |‚ñà‚ñè        | 01:10 elapsed, 08:49 remaining |‚ñà‚ñé        | 01:15 elapsed, 08:44 remaining |‚ñà‚ñé        | 01:15 elapsed, 08:44 remaining |‚ñà‚ñé        | 01:20 elapsed, 08:39 remaining |‚ñà‚ñé        | 01:20 elapsed, 08:39 remaining |‚ñà‚ñç        | 01:25 elapsed, 08:34 remaining |‚ñà‚ñç        | 01:25 elapsed, 08:34 remaining |‚ñà‚ñå        | 01:30 elapsed, 08:29 remaining |‚ñà‚ñå        | 01:30 elapsed, 08:29 remaining |‚ñà‚ñå        | 01:35 elapsed, 08:24 remaining |‚ñà‚ñå        | 01:35 elapsed, 08:24 remainingINFO 12-19 17:57:56 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:57:56 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:57:56 [__init__.py:241] Automatically detected platform cuda.
INFO 12-19 17:57:56 [__init__.py:241] Automatically detected platform cuda.
 |‚ñà‚ñã        | 01:40 elapsed, 08:19 remaining |‚ñà‚ñã        | 01:40 elapsed, 08:19 remaining |‚ñà‚ñä        | 01:45 elapsed, 08:14 remaining |‚ñà‚ñä        | 01:45 elapsed, 08:14 remainingWARNING 12-19 17:58:05 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:58:05 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:58:05 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
WARNING 12-19 17:58:05 [api_server.py:1252] Torch Profiler is enabled in the API server. This should ONLY be used for local development!
 |‚ñà‚ñä        | 01:50 elapsed, 08:09 remaining |‚ñà‚ñä        | 01:50 elapsed, 08:09 remainingINFO 12-19 17:58:11 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_dp2_pp2
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_358c3ea6'), local_subscribe_addr='ipc:///tmp/ad218b72-82ce-4ce7-ad77-bbbc939b290b', remote_subscribe_addr=None, remote_addr_ipv6=False)
 |‚ñà‚ñâ        | 01:55 elapsed, 08:04 remaining |‚ñà‚ñâ        | 01:55 elapsed, 08:04 remainingINFO 12-19 17:58:12 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_dp2_pp2
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_98dbcbc6'), local_subscribe_addr='ipc:///tmp/45f29c2d-224b-4eef-93b4-64decea8d33d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-19 17:58:12 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_dp2_pp2
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1ecfd0fb'), local_subscribe_addr='ipc:///tmp/60e8186e-3d99-467c-a70e-cfb6cd51c659', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-19 17:58:12 [gpu_worker.py:73] Profiling enabled. Traces will be saved to: /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_dp2_pp2
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7e340047'), local_subscribe_addr='ipc:///tmp/6b9a616a-69ce-4481-b2b4-860acc619f9f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:12 [parallel_state.py:992] Adjusting world_size=4 rank=0 distributed_init_method=tcp://127.0.0.1:39783 for DP
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:13 [parallel_state.py:992] Adjusting world_size=4 rank=3 distributed_init_method=tcp://127.0.0.1:39783 for DP
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:13 [parallel_state.py:992] Adjusting world_size=4 rank=1 distributed_init_method=tcp://127.0.0.1:39783 for DP
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:13 [parallel_state.py:992] Adjusting world_size=4 rank=2 distributed_init_method=tcp://127.0.0.1:39783 for DP
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:13 [__init__.py:1426] Found nccl from library libnccl.so.2
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:14 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:14 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:14 [parallel_state.py:1134] rank 3 in world size 4 is assigned as DP rank 1, PP rank 1, TP rank 0, EP rank 1
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:14 [parallel_state.py:1134] rank 1 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[1;36m(VllmWorker PP1 pid=1780881)[0;0m WARNING 12-19 17:58:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP1 pid=1780880)[0;0m WARNING 12-19 17:58:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:14 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:14 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:14 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:14 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:14 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:14 [cuda_communicator.py:86] Using naive all2all manager.
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:14 [parallel_state.py:1134] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:14 [parallel_state.py:1134] rank 2 in world size 4 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 1
[1;36m(VllmWorker PP0 pid=1780879)[0;0m WARNING 12-19 17:58:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP0 pid=1780878)[0;0m WARNING 12-19 17:58:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:14 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:14 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:14 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:14 [gpu_model_runner.py:1929] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:15 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:15 [gpu_model_runner.py:1961] Loading model from scratch...
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:15 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:15 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:15 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:15 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:15 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:15 [weight_utils.py:294] Using model weights format ['*.safetensors']
[1;36m(VllmWorker PP0 pid=1780878)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
 |‚ñà‚ñà        | 02:00 elapsed, 07:59 remaining |‚ñà‚ñà        | 02:00 elapsed, 07:59 remaining[1;36m(VllmWorker PP0 pid=1780878)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.25s/it]
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:19 [default_loader.py:267] Loading weights took 4.35 seconds
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:20 [default_loader.py:267] Loading weights took 4.34 seconds
[1;36m(VllmWorker PP0 pid=1780878)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.02s/it]
[1;36m(VllmWorker PP0 pid=1780878)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.06s/it]
[1;36m(VllmWorker PP0 pid=1780878)[0;0m 
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:20 [default_loader.py:267] Loading weights took 4.29 seconds
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:20 [gpu_model_runner.py:1983] Model loading took 7.5101 GiB and 5.104885 seconds
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:20 [default_loader.py:267] Loading weights took 4.26 seconds
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:20 [gpu_model_runner.py:1983] Model loading took 7.5101 GiB and 5.357876 seconds
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:20 [gpu_model_runner.py:1983] Model loading took 7.5101 GiB and 5.153146 seconds
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:20 [gpu_model_runner.py:1983] Model loading took 7.5101 GiB and 5.378810 seconds
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:58:21 [gpu_worker.py:276] Available KV cache memory: 27.47 GiB
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:58:21 [gpu_worker.py:276] Available KV cache memory: 27.47 GiB
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:58:21 [gpu_worker.py:276] Available KV cache memory: 25.18 GiB
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:58:21 [gpu_worker.py:276] Available KV cache memory: 25.18 GiB
 |‚ñà‚ñà        | 02:05 elapsed, 07:54 remaining |‚ñà‚ñà        | 02:05 elapsed, 07:54 remaining[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:58:21 [kv_cache_utils.py:849] GPU KV cache size: 450,128 tokens
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:58:21 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.43x
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:58:21 [kv_cache_utils.py:849] GPU KV cache size: 412,544 tokens
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:58:21 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.15x
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:58:22 [kv_cache_utils.py:849] GPU KV cache size: 450,128 tokens
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:58:22 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.43x
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:58:22 [kv_cache_utils.py:849] GPU KV cache size: 412,544 tokens
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:58:22 [kv_cache_utils.py:853] Maximum concurrency for 131,072 tokens per request: 3.15x
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:58:22 [core.py:215] init engine (profile, create kv cache, warmup model) took 1.48 seconds
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:58:22 [core.py:215] init engine (profile, create kv cache, warmup model) took 1.26 seconds
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:58:23 [core.py:142] Batch queue is enabled with size 2
[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:58:23 [core.py:142] Batch queue is enabled with size 2
INFO 12-19 17:58:23 [coordinator.py:187] All engine subscriptions received by DP coordinator
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:58:23 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 51568
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:58:23 [loggers.py:142] Engine 001: vllm cache_config_info with initialization after num_gpu_blocks is: 51568
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:58:23 [async_llm.py:151] Torch profiler enabled. AsyncLLM CPU traces will be collected under /pscratch/sd/b/bck/inference_sweep_output/llama3_4gpu_dp2_pp2
 |‚ñà‚ñà‚ñè       | 02:10 elapsed, 07:49 remaining |‚ñà‚ñà‚ñè       | 02:10 elapsed, 07:49 remaining |‚ñà‚ñà‚ñé       | 02:15 elapsed, 07:44 remaining |‚ñà‚ñà‚ñé       | 02:15 elapsed, 07:44 remaining |‚ñà‚ñà‚ñé       | 02:20 elapsed, 07:39 remaining |‚ñà‚ñà‚ñé       | 02:20 elapsed, 07:39 remaining |‚ñà‚ñà‚ñç       | 02:25 elapsed, 07:34 remaining |‚ñà‚ñà‚ñç       | 02:25 elapsed, 07:34 remaining |‚ñà‚ñà‚ñå       | 02:30 elapsed, 07:29 remaining |‚ñà‚ñà‚ñå       | 02:30 elapsed, 07:29 remaining |‚ñà‚ñà‚ñå       | 02:35 elapsed, 07:24 remaining |‚ñà‚ñà‚ñå       | 02:35 elapsed, 07:24 remaining |‚ñà‚ñà‚ñã       | 02:40 elapsed, 07:19 remaining |‚ñà‚ñà‚ñã       | 02:40 elapsed, 07:19 remaining |‚ñà‚ñà‚ñä       | 02:45 elapsed, 07:14 remaining |‚ñà‚ñà‚ñä       | 02:45 elapsed, 07:14 remaining[1;36m(EngineCore_0 pid=1780405)[0;0m INFO 12-19 17:59:01 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(EngineCore_1 pid=1780406)[0;0m INFO 12-19 17:59:02 [__init__.py:3638] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [api_server.py:1679] Supported_tasks: ['generate']
[1;36m(APIServer pid=1779722)[0;0m WARNING 12-19 17:59:02 [__init__.py:1670] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [serving_responses.py:124] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [serving_chat.py:135] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [api_server.py:1950] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:36] Available routes are:
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /start_profile, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /stop_profile, Methods: POST
[1;36m(APIServer pid=1779722)[0;0m INFO 12-19 17:59:02 [launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1779722)[0;0m INFO:     Started server process [1779722]
[1;36m(APIServer pid=1779722)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=1779722)[0;0m INFO:     Application startup complete.
 |‚ñà‚ñà‚ñä       | 02:50 elapsed, 07:09 remaining |‚ñà‚ñà‚ñä       | 02:50 elapsed, 07:09 remaining[1;36m(APIServer pid=1779722)[0;0m INFO:     127.0.0.1:35442 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(VllmWorker PP0 pid=1780878)[0;0m WARNING 12-19 17:59:06 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(VllmWorker PP0 pid=1780878)[0;0m /global/u1/b/bck/vllm/vllm/distributed/parallel_state.py:522: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1577.)
[1;36m(VllmWorker PP0 pid=1780878)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[rank0]:[W1219 17:59:06.855171882 ProcessGroupNCCL.cpp:3632] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[rank1]:[W1219 17:59:06.855925031 ProcessGroupNCCL.cpp:3632] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[1;36m(VllmWorker PP1 pid=1780880)[0;0m WARNING 12-19 17:59:07 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710] EngineCore encountered a fatal error.
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 701, in run_engine_core
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]     engine_core.run_busy_loop()
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 1044, in run_busy_loop
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]     executed = self._process_engine_step()
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 753, in _process_engine_step
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]     outputs, model_executed = self.step_fn()
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 351, in step_with_batch_queue
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]     engine_core_outputs = (self.scheduler.update_from_output(
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/core/sched/scheduler.py", line 831, in update_from_output
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710]     sampled_token_ids = model_runner_output.sampled_token_ids
[1;36m(EngineCore_0 pid=1780405)[0;0m ERROR 12-19 17:59:07 [core.py:710] AttributeError: 'NoneType' object has no attribute 'sampled_token_ids'
[1;36m(VllmWorker PP0 pid=1780878)[0;0m INFO 12-19 17:59:07 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker PP1 pid=1780880)[0;0m INFO 12-19 17:59:07 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453] AsyncLLM output_handler failed.
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453] Traceback (most recent call last):
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453]   File "/global/u1/b/bck/vllm/vllm/v1/engine/async_llm.py", line 412, in output_handler
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453]     outputs = await engine_core.get_output_async()
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core_client.py", line 843, in get_output_async
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453]     raise self._format_exception(outputs) from None
[1;36m(APIServer pid=1779722)[0;0m ERROR 12-19 17:59:07 [async_llm.py:453] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602] WorkerProc hit an exception.
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602] Traceback (most recent call last):
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/v1/executor/multiproc_executor.py", line 597, in worker_busy_loop
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     output = func(*args, **kwargs)
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/v1/worker/gpu_worker.py", line 403, in execute_dummy_batch
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     self.model_runner._dummy_run(1)
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     return func(*args, **kwargs)
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/v1/worker/gpu_model_runner.py", line 2363, in _dummy_run
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     with self.maybe_randomize_inputs(input_ids), set_forward_context(
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/contextlib.py", line 135, in __enter__
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     return next(self.gen)
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/forward_context.py", line 218, in set_forward_context
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     dp_metadata = DPMetadata.make(vllm_config.parallel_config,
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/forward_context.py", line 112, in make
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     num_tokens_across_dp = DPMetadata.num_tokens_across_dp(
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/forward_context.py", line 84, in num_tokens_across_dp
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     dist.all_reduce(num_tokens_tensor, group=get_dp_group().cpu_group)
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     return func(*args, **kwargs)
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2815, in all_reduce
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     work.wait()
[1;36m(VllmWorker PP1 pid=1780881)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [128.55.66.228]:41103
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602] WorkerProc hit an exception.
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602] Traceback (most recent call last):
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/v1/executor/multiproc_executor.py", line 597, in worker_busy_loop
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     output = func(*args, **kwargs)
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/v1/worker/gpu_worker.py", line 403, in execute_dummy_batch
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     self.model_runner._dummy_run(1)
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     return func(*args, **kwargs)
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/v1/worker/gpu_model_runner.py", line 2363, in _dummy_run
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     with self.maybe_randomize_inputs(input_ids), set_forward_context(
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/contextlib.py", line 135, in __enter__
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     return next(self.gen)
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/forward_context.py", line 218, in set_forward_context
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     dp_metadata = DPMetadata.make(vllm_config.parallel_config,
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/forward_context.py", line 112, in make
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     num_tokens_across_dp = DPMetadata.num_tokens_across_dp(
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/u1/b/bck/vllm/vllm/forward_context.py", line 84, in num_tokens_across_dp
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     dist.all_reduce(num_tokens_tensor, group=get_dp_group().cpu_group)
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     return func(*args, **kwargs)
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]   File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2815, in all_reduce
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602]     work.wait()
[1;36m(VllmWorker PP0 pid=1780879)[0;0m ERROR 12-19 17:59:07 [multiproc_executor.py:602] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [128.55.66.228]:40616
[1;36m(APIServer pid=1779722)[0;0m INFO:     Shutting down
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710] EngineCore encountered a fatal error.
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710] Traceback (most recent call last):
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 701, in run_engine_core
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]     engine_core.run_busy_loop()
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 1055, in run_busy_loop
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]     self.execute_dummy_batch()
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/engine/core.py", line 388, in execute_dummy_batch
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]     self.model_executor.collective_rpc("execute_dummy_batch")
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/executor/multiproc_executor.py", line 249, in collective_rpc
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]     result = get_response(w, dequeue_timeout)
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]   File "/global/u1/b/bck/vllm/vllm/v1/executor/multiproc_executor.py", line 236, in get_response
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710]     raise RuntimeError(
[1;36m(EngineCore_1 pid=1780406)[0;0m ERROR 12-19 17:59:07 [core.py:710] RuntimeError: Worker failed with error '[/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [128.55.66.228]:40616', please check the stack trace above for the root cause
[1;36m(VllmWorker PP0 pid=1780879)[0;0m INFO 12-19 17:59:07 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(VllmWorker PP1 pid=1780881)[0;0m INFO 12-19 17:59:07 [multiproc_executor.py:526] Parent process exited, terminating worker
[1;36m(APIServer pid=1779722)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=1779722)[0;0m INFO:     Application shutdown complete.
[1;36m(APIServer pid=1779722)[0;0m INFO:     Finished server process [1779722]
 |‚ñà‚ñà‚ñâ       | 02:55 elapsed, 07:04 remaining |‚ñà‚ñà‚ñâ       | 02:55 elapsed, 07:04 remaining |‚ñà‚ñà‚ñà       | 03:00 elapsed, 06:59 remaining |‚ñà‚ñà‚ñà       | 03:00 elapsed, 06:59 remaining |‚ñà‚ñà‚ñà       | 03:05 elapsed, 06:54 remaining |‚ñà‚ñà‚ñà       | 03:05 elapsed, 06:54 remaining |‚ñà‚ñà‚ñà‚ñè      | 03:10 elapsed, 06:49 remaining |‚ñà‚ñà‚ñà‚ñè      | 03:10 elapsed, 06:49 remaining |‚ñà‚ñà‚ñà‚ñé      | 03:15 elapsed, 06:44 remaining |‚ñà‚ñà‚ñà‚ñé      | 03:15 elapsed, 06:44 remaining |‚ñà‚ñà‚ñà‚ñé      | 03:20 elapsed, 06:39 remaining |‚ñà‚ñà‚ñà‚ñé      | 03:20 elapsed, 06:39 remaining |‚ñà‚ñà‚ñà‚ñç      | 03:25 elapsed, 06:34 remaining |‚ñà‚ñà‚ñà‚ñç      | 03:25 elapsed, 06:34 remaining |‚ñà‚ñà‚ñà‚ñå      | 03:30 elapsed, 06:29 remaining |‚ñà‚ñà‚ñà‚ñå      | 03:30 elapsed, 06:29 remaining |‚ñà‚ñà‚ñà‚ñå      | 03:35 elapsed, 06:24 remaining |‚ñà‚ñà‚ñà‚ñå      | 03:35 elapsed, 06:24 remaining |‚ñà‚ñà‚ñà‚ñã      | 03:40 elapsed, 06:19 remaining |‚ñà‚ñà‚ñà‚ñã      | 03:40 elapsed, 06:19 remaining |‚ñà‚ñà‚ñà‚ñä      | 03:45 elapsed, 06:14 remaining |‚ñà‚ñà‚ñà‚ñä      | 03:45 elapsed, 06:14 remaining |‚ñà‚ñà‚ñà‚ñä      | 03:50 elapsed, 06:09 remaining |‚ñà‚ñà‚ñà‚ñä      | 03:50 elapsed, 06:09 remaining |‚ñà‚ñà‚ñà‚ñâ      | 03:56 elapsed, 06:03 remaining |‚ñà‚ñà‚ñà‚ñâ      | 03:56 elapsed, 06:03 remaining |‚ñà‚ñà‚ñà‚ñà      | 04:01 elapsed, 05:58 remaining |‚ñà‚ñà‚ñà‚ñà      | 04:01 elapsed, 05:58 remaining |‚ñà‚ñà‚ñà‚ñà      | 04:06 elapsed, 05:53 remaining |‚ñà‚ñà‚ñà‚ñà      | 04:06 elapsed, 05:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñè     | 04:11 elapsed, 05:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñè     | 04:11 elapsed, 05:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñé     | 04:16 elapsed, 05:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñé     | 04:16 elapsed, 05:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñé     | 04:21 elapsed, 05:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñé     | 04:21 elapsed, 05:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñç     | 04:26 elapsed, 05:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñç     | 04:26 elapsed, 05:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñå     | 04:31 elapsed, 05:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñå     | 04:31 elapsed, 05:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñå     | 04:36 elapsed, 05:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñå     | 04:36 elapsed, 05:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñã     | 04:41 elapsed, 05:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñã     | 04:41 elapsed, 05:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñä     | 04:46 elapsed, 05:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñä     | 04:46 elapsed, 05:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñä     | 04:51 elapsed, 05:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñä     | 04:51 elapsed, 05:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñâ     | 04:56 elapsed, 05:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñâ     | 04:56 elapsed, 05:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà     | 05:01 elapsed, 04:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà     | 05:01 elapsed, 04:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà     | 05:06 elapsed, 04:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà     | 05:06 elapsed, 04:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 05:11 elapsed, 04:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 05:11 elapsed, 04:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 05:16 elapsed, 04:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 05:16 elapsed, 04:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 05:21 elapsed, 04:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 05:21 elapsed, 04:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 05:26 elapsed, 04:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 05:26 elapsed, 04:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 05:31 elapsed, 04:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 05:31 elapsed, 04:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 05:36 elapsed, 04:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 05:36 elapsed, 04:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 05:41 elapsed, 04:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 05:41 elapsed, 04:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 05:46 elapsed, 04:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 05:46 elapsed, 04:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 05:51 elapsed, 04:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 05:51 elapsed, 04:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 05:56 elapsed, 04:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 05:56 elapsed, 04:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 06:01 elapsed, 03:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 06:01 elapsed, 03:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 06:06 elapsed, 03:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 06:06 elapsed, 03:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 06:11 elapsed, 03:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 06:11 elapsed, 03:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 06:16 elapsed, 03:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 06:16 elapsed, 03:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 06:21 elapsed, 03:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 06:21 elapsed, 03:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 06:26 elapsed, 03:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 06:26 elapsed, 03:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 06:31 elapsed, 03:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 06:31 elapsed, 03:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 06:36 elapsed, 03:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 06:36 elapsed, 03:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 06:41 elapsed, 03:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 06:41 elapsed, 03:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 06:46 elapsed, 03:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 06:46 elapsed, 03:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 06:51 elapsed, 03:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 06:51 elapsed, 03:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 06:56 elapsed, 03:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 06:56 elapsed, 03:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 07:01 elapsed, 02:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 07:01 elapsed, 02:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 07:06 elapsed, 02:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 07:06 elapsed, 02:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 07:11 elapsed, 02:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 07:11 elapsed, 02:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 07:16 elapsed, 02:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 07:16 elapsed, 02:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 07:21 elapsed, 02:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 07:21 elapsed, 02:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 07:26 elapsed, 02:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 07:26 elapsed, 02:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 07:31 elapsed, 02:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 07:31 elapsed, 02:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 07:36 elapsed, 02:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 07:36 elapsed, 02:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 07:41 elapsed, 02:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 07:41 elapsed, 02:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 07:46 elapsed, 02:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 07:46 elapsed, 02:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 07:51 elapsed, 02:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 07:51 elapsed, 02:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 07:56 elapsed, 02:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 07:56 elapsed, 02:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 08:01 elapsed, 01:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 08:01 elapsed, 01:58 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 08:06 elapsed, 01:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 08:06 elapsed, 01:53 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 08:11 elapsed, 01:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 08:11 elapsed, 01:48 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 08:16 elapsed, 01:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 08:16 elapsed, 01:43 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 08:21 elapsed, 01:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 08:21 elapsed, 01:38 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 08:26 elapsed, 01:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 08:26 elapsed, 01:33 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 08:31 elapsed, 01:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 08:31 elapsed, 01:28 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 08:36 elapsed, 01:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 08:36 elapsed, 01:23 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 08:41 elapsed, 01:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 08:41 elapsed, 01:18 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 08:46 elapsed, 01:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 08:46 elapsed, 01:13 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 08:51 elapsed, 01:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 08:51 elapsed, 01:08 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 08:56 elapsed, 01:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 08:56 elapsed, 01:03 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 09:01 elapsed, 00:57 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 09:01 elapsed, 00:57 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 09:07 elapsed, 00:52 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 09:07 elapsed, 00:52 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 09:12 elapsed, 00:47 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 09:12 elapsed, 00:47 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 09:17 elapsed, 00:42 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 09:17 elapsed, 00:42 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 09:22 elapsed, 00:37 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 09:22 elapsed, 00:37 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 09:27 elapsed, 00:32 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 09:27 elapsed, 00:32 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 09:32 elapsed, 00:27 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 09:32 elapsed, 00:27 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 09:37 elapsed, 00:22 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 09:37 elapsed, 00:22 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 09:42 elapsed, 00:17 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 09:42 elapsed, 00:17 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 09:47 elapsed, 00:12 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 09:47 elapsed, 00:12 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 09:52 elapsed, 00:07 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 09:52 elapsed, 00:07 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 09:57 elapsed, 00:02 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 09:57 elapsed, 00:02 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10:00 elapsed, 00:00 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10:00 elapsed, 00:00 remaining |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10:00 elapsed, 00:00 remaining
Traceback (most recent call last):
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/global/u1/b/bck/vllm/vllm/entrypoints/cli/main.py", line 60, in <module>
    main()
  File "/global/u1/b/bck/vllm/vllm/entrypoints/cli/main.py", line 54, in main
    args.dispatch_function(args)
  File "/global/u1/b/bck/vllm/vllm/entrypoints/cli/benchmark/serve.py", line 21, in cmd
    main(args)
  File "/global/u1/b/bck/vllm/vllm/benchmarks/serve.py", line 1063, in main
    return asyncio.run(main_async(args))
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/global/u1/b/bck/vllm/vllm/benchmarks/serve.py", line 1143, in main_async
    benchmark_result = await benchmark(
  File "/global/u1/b/bck/vllm/vllm/benchmarks/serve.py", line 465, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection
    raise first_exception
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection
    sock = await _connect_sock(
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock
    await loop.sock_connect(sock, address)
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/asyncio/selector_events.py", line 501, in sock_connect
    return await fut
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/asyncio/selector_events.py", line 541, in _sock_connect_cb
    raise OSError(err, f'Connect call failed {address}')
ConnectionRefusedError: [Errno 111] Connect call failed ('127.0.0.1', 8000)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/global/u1/b/bck/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 101, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection
    raise last_exc
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
  File "/global/homes/b/bck/.conda/envs/straggler_infer/lib/python3.10/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Connect call failed ('127.0.0.1', 8000)]

