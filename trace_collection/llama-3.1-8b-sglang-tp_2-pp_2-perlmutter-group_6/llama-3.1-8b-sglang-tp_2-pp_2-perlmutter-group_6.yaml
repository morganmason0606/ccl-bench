# workload_template.yaml
version: 1

description: >
  Online serving of Llama-3.1-8B on 4 A100 GPUs with TP=2, PP=2.

hf_url: https://huggingface.co/meta-llama/Llama-3.1-8B
trace_url:

workload:
  model:
    phase: inference
    moe: true
    granularity: model_fwd
    model_family: llama-3.1-8B
    precision: bf16
  data:
    batch_size: 4
    seq_len: 1024
    dataset: random
  hardware:
    network_topo:
      topology: slingshot
      bandwidth_gbps:
        - 200
        - 600
    xpu_spec:
      type: GPU
      model: nvidia_a100
      total_count: 4
      count_per_node: 4
    driver_version: cuda_12.4

Model-executor:
  framework:
    name: sglang
    compiler_tool_selection:
  model_plan_parallelization:
    dp_replicate: 1
    dp_shard: 1
    tp: 2
    pp: 2
    cp: 1
  communication_library:
    name: NCCL
    version: 2.27.3
    env:
      NCCL_IB_QPS_PER_CONNECTION:
  protocol_selection:
    - rocev2
    - p2p

metric_source:
  traces:
    - nsys
  metrics_specific_trace:
    # The jsonl file generated by sglang bench_serving
    - sglang_bench_serving
    # The sqlite file exported from nsys
    - nsys_sqlite
