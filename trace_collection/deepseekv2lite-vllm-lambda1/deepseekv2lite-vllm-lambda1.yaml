# workload_template.yaml
version: 1

description: >
  Online serving of DeepSeek-V2-Lite on 2 A100 GPUs with EP=2.
  pplx kernel is used as AlltoAll backend.
  Script: https://github.com/vllm-project/vllm/blob/main/examples/online_serving/elastic_ep/serve_deepseek_v2.sh

hf_url: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/tree/main
trace_url: https://drive.google.com/drive/u/0/folders/1-Hd-mcvPrcUjHTKOM2iMu5m-RsRefeBH

workload:
  model:
    phase: inference
    moe: true
    granularity: model_fwd
    model_family: deepseek_v2
    precision: bf16
  data:
    batch_size: 2
    seq_len: 8192
    dataset: random
  hardware:
    network_topo:
      topology: 
      bandwidth_gbps:
        - 200
        - 2000
    xpu_spec:
      type: GPU
      model: nvidia_a100
      total_count: 2
      count_per_node: 2
    driver_version: cuda_12.9

Model-executor:
  framework: 
    name: vllm
    compiler_tool_selection: 
  model_plan_parallelization:
    dp_replicate: 1
    dp_shard: 2
    tp: 1
    pp: 1
    cp: 1
  communication_library:
    name: NCCL
    env:
      NCCL_IB_QPS_PER_CONNECTION:
  protocol_selection:
    - rocev2
    - p2p

metric_source:
  traces:
    - torch_et
  metrics_specific_trace:
