# mistral_7b_instruct_v02-megatron_lm-dp_2-tp_2-pp_2-perlmutter-group_1.yaml
version: 1

description: >
  Mistral-7B training on 8 NVIDIA A100 GPUs across 2 nodes on Perlmutter. 
  Configured with hybrid 3D parallelism: 2-way Tensor Parallelism (TP=2), 
  2-way Pipeline Parallelism (PP=2), and 2-way Data Parallelism (DP=2). 
  Includes Sliding Window Attention (SWA) configurations and JIT/Compile disabled.

hf_url: https://huggingface.co/mistralai/Mistral-7B-v0.1
trace_url: https://drive.google.com/file/d/1LM1axibK8bkstW4SjsO3Cu3zQN-KlFd0/view?usp=drive_link

workload:
  model:
    phase: training
    moe: false
    granularity: model_fwd_bwd_pass
    model_family: mistral-7b
    precision: bf16
    epochs: 1
    iteration: 50
  data:
    batch_size: 1
    seq_len: 1024
    dataset: wikitext
  hardware:
    network_topo:
      topology: slingshot
      bandwidth_gbps:
        - 200
        - 2000
    xpu_spec:
      type: GPU
      model: nvidia_a100
      total_count: 8
      count_per_node: 4
    driver_version: cuda_12.4

Model-executor:
  framework: 
    name: megatron-lm
    compiler_tool_selection: plain_pytorch
  model_plan_parallelization:
    dp_replicate: 2 
    dp_shard: 1
    tp: 2          
    pp: 2           
    cp: 1          
  communication_library:
    name: NCCL
    version: 2.21.0
    env:
      NCCL_DEBUG: INFO
      CUDA_DEVICE_MAX_CONNECTIONS: 1
  protocol_selection:
    - rocev2
    - p2p

metric_source:
  traces:
    - kineto_trace
  metrics_specific_trace:
    - memory_trace